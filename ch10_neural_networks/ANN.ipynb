{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris['data'][:, (2,3)], (iris['target'] == 0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X, y)\n",
    "perceptron.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 16:38:17.956567: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 16:38:17.984224: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 16:38:17.984563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 16:38:18.511474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
    "               \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 16:38:42.257975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-03 16:38:42.269329: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x7f0e8b4d0760>,\n",
       " <keras.layers.core.dense.Dense at 0x7f0e4658a530>,\n",
       " <keras.layers.core.dense.Dense at 0x7f0e4658af80>,\n",
       " <keras.layers.core.dense.Dense at 0x7f0e4658a920>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"sgd\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7183 - accuracy: 0.7633 - val_loss: 0.5110 - val_accuracy: 0.8280\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 1s 859us/step - loss: 0.4866 - accuracy: 0.8297 - val_loss: 0.4434 - val_accuracy: 0.8508\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 898us/step - loss: 0.4408 - accuracy: 0.8460 - val_loss: 0.4171 - val_accuracy: 0.8598\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 1s 869us/step - loss: 0.4126 - accuracy: 0.8556 - val_loss: 0.3940 - val_accuracy: 0.8636\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 881us/step - loss: 0.3931 - accuracy: 0.8619 - val_loss: 0.3836 - val_accuracy: 0.8696\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 880us/step - loss: 0.3767 - accuracy: 0.8678 - val_loss: 0.3785 - val_accuracy: 0.8708\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 1s 859us/step - loss: 0.3640 - accuracy: 0.8717 - val_loss: 0.3594 - val_accuracy: 0.8750\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 887us/step - loss: 0.3520 - accuracy: 0.8758 - val_loss: 0.3636 - val_accuracy: 0.8738\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 887us/step - loss: 0.3417 - accuracy: 0.8782 - val_loss: 0.3619 - val_accuracy: 0.8748\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 889us/step - loss: 0.3329 - accuracy: 0.8831 - val_loss: 0.3581 - val_accuracy: 0.8736\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 882us/step - loss: 0.3247 - accuracy: 0.8837 - val_loss: 0.3308 - val_accuracy: 0.8838\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.3168 - accuracy: 0.8859 - val_loss: 0.3295 - val_accuracy: 0.8842\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 0.3080 - accuracy: 0.8891 - val_loss: 0.3273 - val_accuracy: 0.8860\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 931us/step - loss: 0.3022 - accuracy: 0.8913 - val_loss: 0.3327 - val_accuracy: 0.8824\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 880us/step - loss: 0.2947 - accuracy: 0.8941 - val_loss: 0.3244 - val_accuracy: 0.8858\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 894us/step - loss: 0.2892 - accuracy: 0.8954 - val_loss: 0.3248 - val_accuracy: 0.8846\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 904us/step - loss: 0.2834 - accuracy: 0.8973 - val_loss: 0.3229 - val_accuracy: 0.8868\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 888us/step - loss: 0.2777 - accuracy: 0.8991 - val_loss: 0.3112 - val_accuracy: 0.8872\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 905us/step - loss: 0.2716 - accuracy: 0.9012 - val_loss: 0.3078 - val_accuracy: 0.8906\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 897us/step - loss: 0.2678 - accuracy: 0.9037 - val_loss: 0.3009 - val_accuracy: 0.8924\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.2632 - accuracy: 0.9045 - val_loss: 0.3145 - val_accuracy: 0.8888\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 885us/step - loss: 0.2582 - accuracy: 0.9072 - val_loss: 0.3140 - val_accuracy: 0.8892\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 884us/step - loss: 0.2530 - accuracy: 0.9094 - val_loss: 0.3138 - val_accuracy: 0.8884\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 902us/step - loss: 0.2489 - accuracy: 0.9093 - val_loss: 0.3106 - val_accuracy: 0.8894\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 913us/step - loss: 0.2445 - accuracy: 0.9123 - val_loss: 0.3170 - val_accuracy: 0.8884\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 885us/step - loss: 0.2409 - accuracy: 0.9121 - val_loss: 0.3078 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 0.2370 - accuracy: 0.9145 - val_loss: 0.3032 - val_accuracy: 0.8890\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 1s 865us/step - loss: 0.2337 - accuracy: 0.9151 - val_loss: 0.2929 - val_accuracy: 0.8964\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 888us/step - loss: 0.2301 - accuracy: 0.9175 - val_loss: 0.2947 - val_accuracy: 0.8926\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 1s 866us/step - loss: 0.2249 - accuracy: 0.9187 - val_loss: 0.2924 - val_accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 738us/step - loss: 70.2611 - accuracy: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[70.26112365722656, 0.8356000185012817]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/eddie/code/ml/homl/ch10_neural_networks/ANN.ipynb Cell 22\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eddie/code/ml/homl/ch10_neural_networks/ANN.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_classes(X_new)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eddie/code/ml/homl/ch10_neural_networks/ANN.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_pred\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.argmax(y_proba, axis=1)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 882us/step - loss: 1.1956 - val_loss: 0.6605\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 731us/step - loss: 0.5755 - val_loss: 0.5427\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.4705 - val_loss: 0.5065\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 717us/step - loss: 0.4380 - val_loss: 0.4884\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 711us/step - loss: 0.4218 - val_loss: 0.4797\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.4117 - val_loss: 0.4788\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 743us/step - loss: 0.4042 - val_loss: 0.4722\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 711us/step - loss: 0.3984 - val_loss: 0.4663\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.3938 - val_loss: 0.4603\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 654us/step - loss: 0.3897 - val_loss: 0.4629\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.3859 - val_loss: 0.4577\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3827 - val_loss: 0.4565\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3801 - val_loss: 0.4530\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 746us/step - loss: 0.3777 - val_loss: 0.4511\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3752 - val_loss: 0.4504\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.3723 - val_loss: 0.4478\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3709 - val_loss: 0.4458\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 698us/step - loss: 0.3690 - val_loss: 0.4424\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3674 - val_loss: 0.4485\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.3655 - val_loss: 0.4415\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.3642 - val_loss: 0.4432\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 735us/step - loss: 0.3630 - val_loss: 0.4429\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.3617 - val_loss: 0.4379\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 901us/step - loss: 0.3607 - val_loss: 0.4378\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 737us/step - loss: 0.3592 - val_loss: 0.4387\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 737us/step - loss: 0.3580 - val_loss: 0.4385\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 703us/step - loss: 0.3570 - val_loss: 0.4400\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.3558 - val_loss: 0.4376\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.3551 - val_loss: 0.4380\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3538 - val_loss: 0.4403\n",
      "162/162 [==============================] - 0s 449us/step - loss: 1.2434\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=0.005))\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Model (Wide and Deep Net)\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# If we want to have different input features for the deep and wide path\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddie/code/ml/ml_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 973us/step - loss: 2.1066 - val_loss: 0.9649\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.8529 - val_loss: 0.7446\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.7076 - val_loss: 0.6694\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 808us/step - loss: 0.6412 - val_loss: 0.6272\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.5969 - val_loss: 0.5979\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.5638 - val_loss: 0.5754\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 770us/step - loss: 0.5398 - val_loss: 0.5589\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.5199 - val_loss: 0.5463\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.5042 - val_loss: 0.5360\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.4911 - val_loss: 0.5288\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.4798 - val_loss: 0.5213\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.4704 - val_loss: 0.5170\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.4614 - val_loss: 0.5098\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.4535 - val_loss: 0.5070\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.4471 - val_loss: 0.5011\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.4409 - val_loss: 0.4988\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4361 - val_loss: 0.4957\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 802us/step - loss: 0.4317 - val_loss: 0.4911\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.4277 - val_loss: 0.4905\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.4245 - val_loss: 0.4861\n",
      "162/162 [==============================] - 0s 488us/step - loss: 0.5553\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.1000304],\n",
       "       [3.3645573],\n",
       "       [2.085221 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9287 - main_output_loss: 0.8511 - aux_output_loss: 1.6274 - val_loss: 1.5530 - val_main_output_loss: 1.5011 - val_aux_output_loss: 2.0202\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 866us/step - loss: 1.0639 - main_output_loss: 0.9583 - aux_output_loss: 2.0143 - val_loss: 0.6123 - val_main_output_loss: 0.5594 - val_aux_output_loss: 1.0881\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.5209 - main_output_loss: 0.4697 - aux_output_loss: 0.9817 - val_loss: 0.5350 - val_main_output_loss: 0.4902 - val_aux_output_loss: 0.9386\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 889us/step - loss: 0.4539 - main_output_loss: 0.4133 - aux_output_loss: 0.8190 - val_loss: 0.5010 - val_main_output_loss: 0.4639 - val_aux_output_loss: 0.8355\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 884us/step - loss: 0.4251 - main_output_loss: 0.3912 - aux_output_loss: 0.7301 - val_loss: 0.4901 - val_main_output_loss: 0.4595 - val_aux_output_loss: 0.7657\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.4120 - main_output_loss: 0.3831 - aux_output_loss: 0.6723 - val_loss: 0.5020 - val_main_output_loss: 0.4764 - val_aux_output_loss: 0.7323\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 900us/step - loss: 0.4029 - main_output_loss: 0.3768 - aux_output_loss: 0.6381 - val_loss: 0.4838 - val_main_output_loss: 0.4593 - val_aux_output_loss: 0.7041\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.3954 - main_output_loss: 0.3706 - aux_output_loss: 0.6177 - val_loss: 0.4930 - val_main_output_loss: 0.4700 - val_aux_output_loss: 0.7006\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3890 - main_output_loss: 0.3652 - aux_output_loss: 0.6033 - val_loss: 0.5031 - val_main_output_loss: 0.4827 - val_aux_output_loss: 0.6873\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 861us/step - loss: 0.3831 - main_output_loss: 0.3595 - aux_output_loss: 0.5956 - val_loss: 0.5056 - val_main_output_loss: 0.4854 - val_aux_output_loss: 0.6874\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.3776 - main_output_loss: 0.3543 - aux_output_loss: 0.5870 - val_loss: 0.4733 - val_main_output_loss: 0.4509 - val_aux_output_loss: 0.6748\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 877us/step - loss: 0.3743 - main_output_loss: 0.3514 - aux_output_loss: 0.5809 - val_loss: 0.5056 - val_main_output_loss: 0.4851 - val_aux_output_loss: 0.6897\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 866us/step - loss: 0.3712 - main_output_loss: 0.3482 - aux_output_loss: 0.5778 - val_loss: 0.4851 - val_main_output_loss: 0.4639 - val_aux_output_loss: 0.6765\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.3680 - main_output_loss: 0.3451 - aux_output_loss: 0.5735 - val_loss: 0.5067 - val_main_output_loss: 0.4879 - val_aux_output_loss: 0.6761\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.3651 - main_output_loss: 0.3424 - aux_output_loss: 0.5685 - val_loss: 0.4854 - val_main_output_loss: 0.4648 - val_aux_output_loss: 0.6704\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 872us/step - loss: 0.3620 - main_output_loss: 0.3397 - aux_output_loss: 0.5633 - val_loss: 0.4683 - val_main_output_loss: 0.4461 - val_aux_output_loss: 0.6675\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.3613 - main_output_loss: 0.3392 - aux_output_loss: 0.5600 - val_loss: 0.4639 - val_main_output_loss: 0.4425 - val_aux_output_loss: 0.6564\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3588 - main_output_loss: 0.3368 - aux_output_loss: 0.5575 - val_loss: 0.4757 - val_main_output_loss: 0.4541 - val_aux_output_loss: 0.6704\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 881us/step - loss: 0.3562 - main_output_loss: 0.3345 - aux_output_loss: 0.5511 - val_loss: 0.4882 - val_main_output_loss: 0.4685 - val_aux_output_loss: 0.6647\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 886us/step - loss: 0.3543 - main_output_loss: 0.3328 - aux_output_loss: 0.5476 - val_loss: 0.4602 - val_main_output_loss: 0.4386 - val_aux_output_loss: 0.6548\n"
     ]
    }
   ],
   "source": [
    "# Multiple output network\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "\n",
    "# output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1],optimizer=\"sgd\")\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=20, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 629us/step - loss: 1.8285 - main_output_loss: 1.7076 - aux_output_loss: 2.9167\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4f8171a170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.22313079],\n",
       "        [-0.88350517],\n",
       "        [-0.64934933]], dtype=float32),\n",
       " array([[0.5267271 ],\n",
       "        [0.02079654],\n",
       "        [0.09118108]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()\n",
    "model.compile(loss=[\"mse\",\"mse\"], loss_weights=[0.9,0.1], optimizer=\"sgd\")\n",
    "model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3526 - main_output_loss: 0.3314 - aux_output_loss: 0.5434 - val_loss: 0.4797 - val_main_output_loss: 0.4605 - val_aux_output_loss: 0.6533\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.3493 - main_output_loss: 0.3283 - aux_output_loss: 0.5384 - val_loss: 0.4852 - val_main_output_loss: 0.4651 - val_aux_output_loss: 0.6661\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.3488 - main_output_loss: 0.3281 - aux_output_loss: 0.5350 - val_loss: 0.4534 - val_main_output_loss: 0.4330 - val_aux_output_loss: 0.6370\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 847us/step - loss: 0.3481 - main_output_loss: 0.3279 - aux_output_loss: 0.5300 - val_loss: 0.4576 - val_main_output_loss: 0.4368 - val_aux_output_loss: 0.6449\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 846us/step - loss: 0.3449 - main_output_loss: 0.3247 - aux_output_loss: 0.5267 - val_loss: 0.4966 - val_main_output_loss: 0.4789 - val_aux_output_loss: 0.6567\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3430 - main_output_loss: 0.3230 - aux_output_loss: 0.5225 - val_loss: 0.4454 - val_main_output_loss: 0.4253 - val_aux_output_loss: 0.6262\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.3421 - main_output_loss: 0.3226 - aux_output_loss: 0.5181 - val_loss: 0.4447 - val_main_output_loss: 0.4246 - val_aux_output_loss: 0.6259\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3404 - main_output_loss: 0.3211 - aux_output_loss: 0.5140 - val_loss: 0.4609 - val_main_output_loss: 0.4422 - val_aux_output_loss: 0.6295\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 838us/step - loss: 0.3379 - main_output_loss: 0.3189 - aux_output_loss: 0.5088 - val_loss: 0.4477 - val_main_output_loss: 0.4282 - val_aux_output_loss: 0.6233\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3374 - main_output_loss: 0.3188 - aux_output_loss: 0.5049 - val_loss: 0.4851 - val_main_output_loss: 0.4682 - val_aux_output_loss: 0.6367\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.3349 - main_output_loss: 0.3163 - aux_output_loss: 0.5022 - val_loss: 0.4274 - val_main_output_loss: 0.4084 - val_aux_output_loss: 0.5988\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3337 - main_output_loss: 0.3156 - aux_output_loss: 0.4965 - val_loss: 0.4505 - val_main_output_loss: 0.4324 - val_aux_output_loss: 0.6137\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3325 - main_output_loss: 0.3146 - aux_output_loss: 0.4938 - val_loss: 0.4479 - val_main_output_loss: 0.4301 - val_aux_output_loss: 0.6078\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3314 - main_output_loss: 0.3138 - aux_output_loss: 0.4897 - val_loss: 0.4460 - val_main_output_loss: 0.4282 - val_aux_output_loss: 0.6063\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3306 - main_output_loss: 0.3132 - aux_output_loss: 0.4866 - val_loss: 0.4456 - val_main_output_loss: 0.4276 - val_aux_output_loss: 0.6081\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3296 - main_output_loss: 0.3126 - aux_output_loss: 0.4825 - val_loss: 0.4530 - val_main_output_loss: 0.4355 - val_aux_output_loss: 0.6097\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.3289 - main_output_loss: 0.3122 - aux_output_loss: 0.4791 - val_loss: 0.4467 - val_main_output_loss: 0.4298 - val_aux_output_loss: 0.5991\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 878us/step - loss: 0.3273 - main_output_loss: 0.3108 - aux_output_loss: 0.4763 - val_loss: 0.4422 - val_main_output_loss: 0.4251 - val_aux_output_loss: 0.5965\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.3262 - main_output_loss: 0.3101 - aux_output_loss: 0.4716 - val_loss: 0.4555 - val_main_output_loss: 0.4402 - val_aux_output_loss: 0.5934\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.3239 - main_output_loss: 0.3079 - aux_output_loss: 0.4679 - val_loss: 0.4650 - val_main_output_loss: 0.4495 - val_aux_output_loss: 0.6043\n"
     ]
    }
   ],
   "source": [
    "# Saving a model\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1],optimizer=\"sgd\")\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=20, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "363/363 [==============================] - 0s 880us/step - loss: 1.3152 - val_loss: 0.6204\n",
      "Epoch 2/400\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.5427 - val_loss: 0.5404\n",
      "Epoch 3/400\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.4678 - val_loss: 0.5132\n",
      "Epoch 4/400\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.4422 - val_loss: 0.4985\n",
      "Epoch 5/400\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.4267 - val_loss: 0.4908\n",
      "Epoch 6/400\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.4155 - val_loss: 0.4771\n",
      "Epoch 7/400\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.4055 - val_loss: 0.4710\n",
      "Epoch 8/400\n",
      "363/363 [==============================] - 0s 720us/step - loss: 0.3972 - val_loss: 0.4671\n",
      "Epoch 9/400\n",
      "363/363 [==============================] - 0s 723us/step - loss: 0.3926 - val_loss: 0.4600\n",
      "Epoch 10/400\n",
      "363/363 [==============================] - 0s 720us/step - loss: 0.3875 - val_loss: 0.4619\n",
      "Epoch 11/400\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.3832 - val_loss: 0.4581\n",
      "Epoch 12/400\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.3804 - val_loss: 0.4513\n",
      "Epoch 13/400\n",
      "363/363 [==============================] - 0s 742us/step - loss: 0.3761 - val_loss: 0.4493\n",
      "Epoch 14/400\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.3743 - val_loss: 0.4457\n",
      "Epoch 15/400\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.3709 - val_loss: 0.4467\n",
      "Epoch 16/400\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.3689 - val_loss: 0.4397\n",
      "Epoch 17/400\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.3673 - val_loss: 0.4445\n",
      "Epoch 18/400\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3655 - val_loss: 0.4362\n",
      "Epoch 19/400\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.3634 - val_loss: 0.4362\n",
      "Epoch 20/400\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.3614 - val_loss: 0.4306\n",
      "Epoch 21/400\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3604 - val_loss: 0.4362\n",
      "Epoch 22/400\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.3585 - val_loss: 0.4332\n",
      "Epoch 23/400\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3576 - val_loss: 0.4300\n",
      "Epoch 24/400\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.3552 - val_loss: 0.4293\n",
      "Epoch 25/400\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.3544 - val_loss: 0.4290\n",
      "Epoch 26/400\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3529 - val_loss: 0.4347\n",
      "Epoch 27/400\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3520 - val_loss: 0.4270\n",
      "Epoch 28/400\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.3501 - val_loss: 0.4259\n",
      "Epoch 29/400\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.3495 - val_loss: 0.4259\n",
      "Epoch 30/400\n",
      "363/363 [==============================] - 0s 665us/step - loss: 0.3480 - val_loss: 0.4273\n",
      "Epoch 31/400\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3476 - val_loss: 0.4243\n",
      "Epoch 32/400\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.3456 - val_loss: 0.4317\n",
      "Epoch 33/400\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.3460 - val_loss: 0.4187\n",
      "Epoch 34/400\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3445 - val_loss: 0.4272\n",
      "Epoch 35/400\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.3442 - val_loss: 0.4241\n",
      "Epoch 36/400\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.3421 - val_loss: 0.4159\n",
      "Epoch 37/400\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3417 - val_loss: 0.4183\n",
      "Epoch 38/400\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.3408 - val_loss: 0.4183\n",
      "Epoch 39/400\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.3399 - val_loss: 0.4230\n",
      "Epoch 40/400\n",
      "363/363 [==============================] - 0s 720us/step - loss: 0.3396 - val_loss: 0.4267\n",
      "Epoch 41/400\n",
      "363/363 [==============================] - 0s 676us/step - loss: 0.3443 - val_loss: 0.4159\n",
      "Epoch 42/400\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3387 - val_loss: 0.4227\n",
      "Epoch 43/400\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.3368 - val_loss: 0.4113\n",
      "Epoch 44/400\n",
      "363/363 [==============================] - 0s 714us/step - loss: 0.3366 - val_loss: 0.4153\n",
      "Epoch 45/400\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3352 - val_loss: 0.4201\n",
      "Epoch 46/400\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3355 - val_loss: 0.4125\n",
      "Epoch 47/400\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3359 - val_loss: 0.4177\n",
      "Epoch 48/400\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.3329 - val_loss: 0.4167\n",
      "Epoch 49/400\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.3324 - val_loss: 0.4207\n",
      "Epoch 50/400\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3323 - val_loss: 0.4087\n",
      "Epoch 51/400\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.3316 - val_loss: 0.4150\n",
      "Epoch 52/400\n",
      "363/363 [==============================] - 0s 698us/step - loss: 0.3316 - val_loss: 0.4113\n",
      "Epoch 53/400\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3312 - val_loss: 0.4165\n",
      "Epoch 54/400\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.3299 - val_loss: 0.4103\n",
      "Epoch 55/400\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.3302 - val_loss: 0.4147\n",
      "Epoch 56/400\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.3307 - val_loss: 0.4154\n",
      "Epoch 57/400\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3292 - val_loss: 0.4241\n",
      "Epoch 58/400\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3295 - val_loss: 0.4103\n",
      "Epoch 59/400\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.3275 - val_loss: 0.4085\n",
      "Epoch 60/400\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3280 - val_loss: 0.4126\n",
      "Epoch 61/400\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3284 - val_loss: 0.4126\n",
      "Epoch 62/400\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3280 - val_loss: 0.4115\n",
      "Epoch 63/400\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3268 - val_loss: 0.4039\n",
      "Epoch 64/400\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.3260 - val_loss: 0.4170\n",
      "Epoch 65/400\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3248 - val_loss: 0.4095\n",
      "Epoch 66/400\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3246 - val_loss: 0.4076\n",
      "Epoch 67/400\n",
      "363/363 [==============================] - 0s 676us/step - loss: 0.3257 - val_loss: 0.4137\n",
      "Epoch 68/400\n",
      "363/363 [==============================] - 0s 714us/step - loss: 0.3244 - val_loss: 0.4161\n",
      "Epoch 69/400\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.3232 - val_loss: 0.4133\n",
      "Epoch 70/400\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3234 - val_loss: 0.4100\n",
      "Epoch 71/400\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.3246 - val_loss: 0.4034\n",
      "Epoch 72/400\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.3234 - val_loss: 0.4214\n",
      "Epoch 73/400\n",
      "363/363 [==============================] - 0s 689us/step - loss: 0.3242 - val_loss: 0.4128\n",
      "Epoch 74/400\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.3222 - val_loss: 0.4063\n",
      "Epoch 75/400\n",
      "363/363 [==============================] - 0s 678us/step - loss: 0.3221 - val_loss: 0.4091\n",
      "Epoch 76/400\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.3224 - val_loss: 0.4049\n",
      "Epoch 77/400\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3210 - val_loss: 0.4150\n",
      "Epoch 78/400\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.3218 - val_loss: 0.4043\n",
      "Epoch 79/400\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.3199 - val_loss: 0.4058\n",
      "Epoch 80/400\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.3190 - val_loss: 0.4047\n",
      "Epoch 81/400\n",
      "363/363 [==============================] - 0s 723us/step - loss: 0.3184 - val_loss: 0.4064\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=0.005))\n",
    "\n",
    "# Only works with Sequential and Functional API, not Subclassing API\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\")\n",
    "# history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
    "\n",
    "# Save best only, will run all epochs and save the model with the best validation performance\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True)\n",
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid,y_valid), callbacks=[checkpoint_cb])\n",
    "# model = keras.models.load_model(\"my_model.h5\") # roll back to best model\n",
    "\n",
    "\n",
    "# Early Stopping (will stop training when validation performance has not improved for <patience> epochs)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=400, validation_data=(X_valid,y_valid), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback example\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "    def on_epoch_begin():\n",
    "        pass\n",
    "    def on_train_begin():\n",
    "        pass\n",
    "    def on_train_end():\n",
    "        pass\n",
    "    def on_batch_begin():\n",
    "        pass\n",
    "    def on_batch_end():\n",
    "        pass\n",
    "\n",
    "    # called by evaluate\n",
    "    def on_test_begin():\n",
    "        pass\n",
    "    def on_test_end():\n",
    "        pass\n",
    "    def on_test_batch_begin():\n",
    "        pass\n",
    "    def on_test_batch_end():\n",
    "        pass\n",
    "\n",
    "    # called by predict\n",
    "    def on_predict_begin():\n",
    "        pass\n",
    "    def on_predict_end():\n",
    "        pass\n",
    "    def on_predict_batch_begin():\n",
    "        pass\n",
    "    def on_predict_batch_end():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 860us/step - loss: 1.5333 - val_loss: 0.6861\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.5841 - val_loss: 0.5746\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.4909 - val_loss: 0.5272\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4527 - val_loss: 0.5060\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.4334 - val_loss: 0.4959\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.4206 - val_loss: 0.4908\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.4113 - val_loss: 0.4810\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.4047 - val_loss: 0.4804\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3981 - val_loss: 0.4736\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.3924 - val_loss: 0.4693\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3896 - val_loss: 0.4658\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 643us/step - loss: 0.3852 - val_loss: 0.4597\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.3820 - val_loss: 0.4554\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3793 - val_loss: 0.4559\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3765 - val_loss: 0.4604\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3744 - val_loss: 0.4522\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3705 - val_loss: 0.4465\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 654us/step - loss: 0.3694 - val_loss: 0.4480\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3677 - val_loss: 0.4448\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.3649 - val_loss: 0.4501\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.3640 - val_loss: 0.4413\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3623 - val_loss: 0.4391\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.3599 - val_loss: 0.4413\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3588 - val_loss: 0.4337\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 689us/step - loss: 0.3585 - val_loss: 0.4349\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3555 - val_loss: 0.4369\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 700us/step - loss: 0.3547 - val_loss: 0.4370\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 703us/step - loss: 0.3534 - val_loss: 0.4321\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3527 - val_loss: 0.4319\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3519 - val_loss: 0.4334\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=0.005))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.3504 - val_loss: 0.4393\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.3491 - val_loss: 0.4469\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3491 - val_loss: 0.4368\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3477 - val_loss: 0.4327\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3467 - val_loss: 0.4311\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.3475 - val_loss: 0.4272\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3454 - val_loss: 0.4288\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.3437 - val_loss: 0.4292\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.3432 - val_loss: 0.4308\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.3429 - val_loss: 0.4262\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 700us/step - loss: 0.3417 - val_loss: 0.4379\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.3417 - val_loss: 0.4334\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.3399 - val_loss: 0.4346\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.3396 - val_loss: 0.4221\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.3394 - val_loss: 0.4237\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3382 - val_loss: 0.4307\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 731us/step - loss: 0.3385 - val_loss: 0.4289\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3371 - val_loss: 0.4294\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.3370 - val_loss: 0.4195\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.3363 - val_loss: 0.4205\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.3360 - val_loss: 0.4219\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 646us/step - loss: 0.3348 - val_loss: 0.4213\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.3348 - val_loss: 0.4317\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3338 - val_loss: 0.4296\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.3351 - val_loss: 0.4132\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.3323 - val_loss: 0.4236\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3332 - val_loss: 0.4332\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 675us/step - loss: 0.3314 - val_loss: 0.4261\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 723us/step - loss: 0.3312 - val_loss: 0.4212\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.3306 - val_loss: 0.4152\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32x32 rgb images\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 46s - loss: 3.8471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1885063/110609044.py:11: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
      "/home/eddie/code/ml/ml_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 846us/step - loss: 1.1780 - val_loss: 0.7440\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 668us/step - loss: 0.6472 - val_loss: 0.6372\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.5532 - val_loss: 0.5798\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.5112 - val_loss: 0.5543\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.4853 - val_loss: 0.5363\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.4648 - val_loss: 0.5230\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.4499 - val_loss: 0.5100\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.4364 - val_loss: 0.5042\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 711us/step - loss: 0.4256 - val_loss: 0.4933\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.4165 - val_loss: 0.4833\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.4102 - val_loss: 0.4802\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.4034 - val_loss: 0.4744\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3976 - val_loss: 0.4713\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.3927 - val_loss: 0.4670\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.3889 - val_loss: 0.4623\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3853 - val_loss: 0.4551\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 703us/step - loss: 0.3824 - val_loss: 0.4532\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 717us/step - loss: 0.3795 - val_loss: 0.4525\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 689us/step - loss: 0.3770 - val_loss: 0.4479\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3745 - val_loss: 0.4484\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.3727 - val_loss: 0.4471\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3703 - val_loss: 0.4472\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.3684 - val_loss: 0.4454\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.3663 - val_loss: 0.4448\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3649 - val_loss: 0.4425\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 671us/step - loss: 0.3630 - val_loss: 0.4380\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 714us/step - loss: 0.3613 - val_loss: 0.4396\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.3600 - val_loss: 0.4369\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.3589 - val_loss: 0.4364\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 717us/step - loss: 0.3574 - val_loss: 0.4347\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.3565 - val_loss: 0.4358\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3550 - val_loss: 0.4392\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.3537 - val_loss: 0.4299\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.3526 - val_loss: 0.4325\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.3514 - val_loss: 0.4326\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.3514 - val_loss: 0.4318\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 662us/step - loss: 0.3498 - val_loss: 0.4302\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.3495 - val_loss: 0.4292\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.3490 - val_loss: 0.4279\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 686us/step - loss: 0.3485 - val_loss: 0.4279\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3475 - val_loss: 0.4263\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.3466 - val_loss: 0.4284\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 681us/step - loss: 0.3470 - val_loss: 0.4274\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3458 - val_loss: 0.4266\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.3451 - val_loss: 0.4232\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3449 - val_loss: 0.4262\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.3451 - val_loss: 0.4245\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3439 - val_loss: 0.4273\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.3432 - val_loss: 0.4249\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.3432 - val_loss: 0.4233\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 711us/step - loss: 0.3427 - val_loss: 0.4259\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 720us/step - loss: 0.3427 - val_loss: 0.4221\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.3418 - val_loss: 0.4259\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3414 - val_loss: 0.4246\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.3409 - val_loss: 0.4243\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.3413 - val_loss: 0.4228\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.3409 - val_loss: 0.4215\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3404 - val_loss: 0.4202\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.3399 - val_loss: 0.4198\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 700us/step - loss: 0.3395 - val_loss: 0.4221\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3393 - val_loss: 0.4184\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3386 - val_loss: 0.4193\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 648us/step - loss: 0.3385 - val_loss: 0.4208\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 700us/step - loss: 0.3382 - val_loss: 0.4261\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.3377 - val_loss: 0.4201\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3377 - val_loss: 0.4220\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3375 - val_loss: 0.4210\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3377 - val_loss: 0.4158\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.3369 - val_loss: 0.4246\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.3372 - val_loss: 0.4187\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 665us/step - loss: 0.3368 - val_loss: 0.4230\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.3362 - val_loss: 0.4192\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3358 - val_loss: 0.4220\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.3361 - val_loss: 0.4165\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.3355 - val_loss: 0.4244\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.3357 - val_loss: 0.4176\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.3351 - val_loss: 0.4199\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3350 - val_loss: 0.4214\n",
      "162/162 [==============================] - 0s 469us/step - loss: 1.5249\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 39s - loss: 5.2778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddie/code/ml/ml_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5069 - val_loss: 0.8133\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.7313 - val_loss: 0.6873\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.6381 - val_loss: 0.6349\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5802 - val_loss: 0.5851\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.5295 - val_loss: 0.5500\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4900 - val_loss: 0.5226\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4598 - val_loss: 0.5032\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4393 - val_loss: 0.4896\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4237 - val_loss: 0.4813\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.4144 - val_loss: 0.4780\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4069 - val_loss: 0.4731\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3995 - val_loss: 0.4611\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3926 - val_loss: 0.4592\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3857 - val_loss: 0.4618\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3833 - val_loss: 0.4601\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3798 - val_loss: 0.4523\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3760 - val_loss: 0.4467\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3729 - val_loss: 0.4474\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3708 - val_loss: 0.4393\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3688 - val_loss: 0.4392\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3664 - val_loss: 0.4372\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3651 - val_loss: 0.4340\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3639 - val_loss: 0.4355\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3621 - val_loss: 0.4326\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3603 - val_loss: 0.4353\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3595 - val_loss: 0.4301\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3586 - val_loss: 0.4333\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3567 - val_loss: 0.4327\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3558 - val_loss: 0.4337\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.3541 - val_loss: 0.4326\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3536 - val_loss: 0.4306\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3530 - val_loss: 0.4294\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3510 - val_loss: 0.4327\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3509 - val_loss: 0.4276\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3501 - val_loss: 0.4250\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3490 - val_loss: 0.4302\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3482 - val_loss: 0.4279\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3470 - val_loss: 0.4246\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3466 - val_loss: 0.4273\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3464 - val_loss: 0.4242\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3458 - val_loss: 0.4304\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3451 - val_loss: 0.4289\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3449 - val_loss: 0.4270\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3440 - val_loss: 0.4252\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3433 - val_loss: 0.4254\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3426 - val_loss: 0.4237\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3435 - val_loss: 0.4225\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3419 - val_loss: 0.4235\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3410 - val_loss: 0.4202\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3407 - val_loss: 0.4301\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3410 - val_loss: 0.4233\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3399 - val_loss: 0.4363\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3394 - val_loss: 0.4231\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3391 - val_loss: 0.4293\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3385 - val_loss: 0.4226\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.3384 - val_loss: 0.4226\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3378 - val_loss: 0.4297\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3365 - val_loss: 0.4435\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3373 - val_loss: 0.4324\n",
      "121/121 [==============================] - 0s 486us/step - loss: 0.3359\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1876 - val_loss: 0.7143\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.6343 - val_loss: 0.6426\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5756 - val_loss: 0.5993\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.5329 - val_loss: 0.5710\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.4965 - val_loss: 0.5418\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4699 - val_loss: 0.5266\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4507 - val_loss: 0.5092\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.4360 - val_loss: 0.5006\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4247 - val_loss: 0.4940\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4159 - val_loss: 0.4845\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.4079 - val_loss: 0.4732\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4011 - val_loss: 0.4758\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3945 - val_loss: 0.4751\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3896 - val_loss: 0.4620\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3846 - val_loss: 0.4671\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3813 - val_loss: 0.4561\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3772 - val_loss: 0.4549\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3750 - val_loss: 0.4524\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3726 - val_loss: 0.4562\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3704 - val_loss: 0.4549\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3682 - val_loss: 0.4488\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3658 - val_loss: 0.4488\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3647 - val_loss: 0.4443\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3638 - val_loss: 0.4469\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3622 - val_loss: 0.4473\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3606 - val_loss: 0.4459\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3589 - val_loss: 0.4398\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3588 - val_loss: 0.4490\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3573 - val_loss: 0.4389\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3566 - val_loss: 0.4380\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3560 - val_loss: 0.4388\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3555 - val_loss: 0.4405\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3548 - val_loss: 0.4408\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3536 - val_loss: 0.4400\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3527 - val_loss: 0.4449\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3516 - val_loss: 0.4431\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3505 - val_loss: 0.4375\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3510 - val_loss: 0.4391\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3499 - val_loss: 0.4422\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3490 - val_loss: 0.4371\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3491 - val_loss: 0.4406\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3475 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3471 - val_loss: 0.4399\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.3464 - val_loss: 0.4385\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3465 - val_loss: 0.4381\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3456 - val_loss: 0.4355\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3450 - val_loss: 0.4330\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3437 - val_loss: 0.4330\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3437 - val_loss: 0.4318\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3434 - val_loss: 0.4334\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3426 - val_loss: 0.4448\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3427 - val_loss: 0.4350\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3421 - val_loss: 0.4405\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3414 - val_loss: 0.4365\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3407 - val_loss: 0.4390\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3403 - val_loss: 0.4406\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3397 - val_loss: 0.4395\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3397 - val_loss: 0.4318\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3390 - val_loss: 0.4336\n",
      "121/121 [==============================] - 0s 460us/step - loss: 0.3465\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8373 - val_loss: 1.0092\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.8593 - val_loss: 0.6823\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.6647 - val_loss: 0.6172\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.6027 - val_loss: 0.5789\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.5629 - val_loss: 0.5543\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5331 - val_loss: 0.5472\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.5120 - val_loss: 0.5315\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4935 - val_loss: 0.5223\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4778 - val_loss: 0.5158\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4644 - val_loss: 0.5116\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4521 - val_loss: 0.4957\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4416 - val_loss: 0.4867\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4326 - val_loss: 0.4862\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4237 - val_loss: 0.4868\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4155 - val_loss: 0.4758\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4096 - val_loss: 0.4668\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4026 - val_loss: 0.4650\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3965 - val_loss: 0.4624\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3907 - val_loss: 0.4637\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3859 - val_loss: 0.4505\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3802 - val_loss: 0.4566\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3763 - val_loss: 0.4489\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3719 - val_loss: 0.4435\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3678 - val_loss: 0.4432\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3634 - val_loss: 0.4400\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3603 - val_loss: 0.4389\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3561 - val_loss: 0.4400\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.3524 - val_loss: 0.4319\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3506 - val_loss: 0.4339\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3461 - val_loss: 0.4311\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3445 - val_loss: 0.4310\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3430 - val_loss: 0.4302\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3387 - val_loss: 0.4249\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3380 - val_loss: 0.4259\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3364 - val_loss: 0.4250\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.3337 - val_loss: 0.4185\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3321 - val_loss: 0.4197\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3296 - val_loss: 0.4308\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3294 - val_loss: 0.4197\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3282 - val_loss: 0.4201\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3269 - val_loss: 0.4199\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3275 - val_loss: 0.4205\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3244 - val_loss: 0.4195\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3229 - val_loss: 0.4156\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3250 - val_loss: 0.4172\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3234 - val_loss: 0.4189\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3209 - val_loss: 0.4137\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3289 - val_loss: 0.4120\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3229 - val_loss: 0.4173\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3187 - val_loss: 0.4176\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3179 - val_loss: 0.4163\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3174 - val_loss: 0.4194\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3164 - val_loss: 0.4214\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3176 - val_loss: 0.4144\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3139 - val_loss: 0.4131\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3194 - val_loss: 0.4146\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3141 - val_loss: 0.4140\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3132 - val_loss: 0.4186\n",
      "121/121 [==============================] - 0s 465us/step - loss: 0.3338\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1668 - val_loss: 0.5772\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.4891 - val_loss: 0.5254\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.4429 - val_loss: 0.4971\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4237 - val_loss: 0.4802\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.4122 - val_loss: 0.4869\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4044 - val_loss: 0.4723\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4009 - val_loss: 0.4686\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3957 - val_loss: 0.4795\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3939 - val_loss: 0.4592\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3911 - val_loss: 0.4611\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3894 - val_loss: 0.4579\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3861 - val_loss: 0.4574\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3841 - val_loss: 0.4554\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3822 - val_loss: 0.4597\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.3804 - val_loss: 0.4712\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3771 - val_loss: 0.4528\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3775 - val_loss: 0.4433\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3757 - val_loss: 0.4415\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3732 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3724 - val_loss: 0.4455\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3729 - val_loss: 0.4598\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3712 - val_loss: 0.4453\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3717 - val_loss: 0.4458\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3703 - val_loss: 0.4553\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3704 - val_loss: 0.4445\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3693 - val_loss: 0.4448\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3689 - val_loss: 0.4534\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.3668 - val_loss: 0.4568\n",
      "121/121 [==============================] - 0s 532us/step - loss: 0.3754\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8578 - val_loss: 0.6191\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.5121 - val_loss: 0.5692\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4703 - val_loss: 0.5355\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4535 - val_loss: 0.5200\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.4445 - val_loss: 0.5260\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4346 - val_loss: 0.5022\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.4287 - val_loss: 0.4956\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4302 - val_loss: 0.4918\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4212 - val_loss: 0.4872\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.4169 - val_loss: 0.4960\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4178 - val_loss: 0.4800\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4112 - val_loss: 0.4858\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4150 - val_loss: 0.4852\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.4099 - val_loss: 0.4752\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4074 - val_loss: 0.4800\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.4044 - val_loss: 0.4704\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4164 - val_loss: 0.4685\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4027 - val_loss: 0.4629\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3967 - val_loss: 0.4920\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4020 - val_loss: 0.4733\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3977 - val_loss: 0.4649\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4026 - val_loss: 0.4776\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3917 - val_loss: 0.4609\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.3874 - val_loss: 0.4630\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3866 - val_loss: 0.4652\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3838 - val_loss: 0.4700\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3827 - val_loss: 0.4615\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3836 - val_loss: 0.4677\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3807 - val_loss: 0.4623\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3787 - val_loss: 0.4578\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3773 - val_loss: 0.4592\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3779 - val_loss: 0.4605\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3760 - val_loss: 0.4520\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3786 - val_loss: 0.4539\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3758 - val_loss: 0.4580\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3781 - val_loss: 0.4591\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3778 - val_loss: 0.4599\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3763 - val_loss: 0.4529\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3752 - val_loss: 0.4525\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3758 - val_loss: 0.4712\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3795 - val_loss: 0.4625\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3765 - val_loss: 0.4619\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3751 - val_loss: 0.4607\n",
      "121/121 [==============================] - 0s 479us/step - loss: 0.3758\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8754 - val_loss: 0.6052\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.5119 - val_loss: 0.5508\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4641 - val_loss: 0.5268\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.4401 - val_loss: 0.5057\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4282 - val_loss: 0.4916\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4220 - val_loss: 0.4868\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4179 - val_loss: 0.4825\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.4117 - val_loss: 0.4775\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4104 - val_loss: 0.4721\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.4069 - val_loss: 0.4893\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.4037 - val_loss: 0.4727\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3982 - val_loss: 0.4654\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3955 - val_loss: 0.4746\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3902 - val_loss: 0.4723\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3854 - val_loss: 0.4615\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3857 - val_loss: 0.4601\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3801 - val_loss: 0.4529\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3778 - val_loss: 0.4566\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3780 - val_loss: 0.4614\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3759 - val_loss: 0.4531\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3735 - val_loss: 0.4495\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3731 - val_loss: 0.4519\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3734 - val_loss: 0.4698\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3722 - val_loss: 0.4607\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3710 - val_loss: 0.4522\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3731 - val_loss: 0.4532\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3686 - val_loss: 0.4462\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3687 - val_loss: 0.4656\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3678 - val_loss: 0.4510\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3652 - val_loss: 0.4513\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.3664 - val_loss: 0.4568\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3655 - val_loss: 0.4458\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3638 - val_loss: 0.4490\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3673 - val_loss: 0.4592\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3635 - val_loss: 0.4391\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3627 - val_loss: 0.4388\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3620 - val_loss: 0.4469\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3642 - val_loss: 0.4533\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3602 - val_loss: 0.4629\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3615 - val_loss: 0.4664\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3597 - val_loss: 0.4489\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3579 - val_loss: 0.4466\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3577 - val_loss: 0.4493\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3588 - val_loss: 0.4398\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3598 - val_loss: 0.4573\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.3595 - val_loss: 0.4457\n",
      "121/121 [==============================] - 0s 487us/step - loss: 0.3714\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 1.2580 - val_loss: 1.5892\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6225 - val_loss: 5.5911\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 2.3500 - val_loss: 197.2605\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 111.1005 - val_loss: 4183.3403\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 25115.9160 - val_loss: 97958.7812\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 32082.7734 - val_loss: 2322794.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 706644.2500 - val_loss: 53680784.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 298903232.0000 - val_loss: 1225952256.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 2115343744.0000 - val_loss: 28075036672.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 41732943872.0000 - val_loss: 631604969472.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 1170771410944.0000 - val_loss: 14249765109760.0000\n",
      "121/121 [==============================] - 0s 472us/step - loss: 7225510199296.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.9452 - val_loss: 0.5930\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5519 - val_loss: 0.5655\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5194 - val_loss: 0.5806\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.5536 - val_loss: 1.2068\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.5981 - val_loss: 1.1118\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.6650 - val_loss: 1.9362\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.7814 - val_loss: 2.0213\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.9110 - val_loss: 2.1414\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.7496 - val_loss: 1.3727\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.8943 - val_loss: 1.1758\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.6344 - val_loss: 1.5354\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.9369 - val_loss: 1.6633\n",
      "121/121 [==============================] - 0s 473us/step - loss: 2.9190\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 1.3036 - val_loss: 14.7126\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 20.4491 - val_loss: 1346.7960\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 1621.8154 - val_loss: 130661.5078\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 2185976.5000 - val_loss: 12572143.0000\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 138110944.0000 - val_loss: 1185006720.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 44647530496.0000 - val_loss: 110649122816.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 1689877479424.0000 - val_loss: 11662155317248.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 22222215315456.0000 - val_loss: 984429321256960.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 16905177034391552.0000 - val_loss: 94960979590250496.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 90634899551158272.0000 - val_loss: 9577375198580047872.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 135304185675975229440.0000 - val_loss: 921643719711606702080.0000\n",
      "121/121 [==============================] - 0s 440us/step - loss: 60913529118976376832.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1109 - val_loss: 1.4639\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 1.1486 - val_loss: 0.9008\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.8155 - val_loss: 0.7602\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.7189 - val_loss: 0.7033\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.6763 - val_loss: 0.6747\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.6515 - val_loss: 0.6568\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.6329 - val_loss: 0.6425\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6164 - val_loss: 0.6310\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.6023 - val_loss: 0.6200\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.5895 - val_loss: 0.6106\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.5779 - val_loss: 0.6024\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.5668 - val_loss: 0.5940\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.5561 - val_loss: 0.5872\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.5464 - val_loss: 0.5798\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5369 - val_loss: 0.5738\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.5280 - val_loss: 0.5671\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5190 - val_loss: 0.5605\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.5105 - val_loss: 0.5561\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.5027 - val_loss: 0.5498\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4947 - val_loss: 0.5454\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.4877 - val_loss: 0.5388\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4803 - val_loss: 0.5338\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4740 - val_loss: 0.5288\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4674 - val_loss: 0.5240\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4611 - val_loss: 0.5194\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4555 - val_loss: 0.5157\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4497 - val_loss: 0.5112\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4445 - val_loss: 0.5084\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4395 - val_loss: 0.5040\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4348 - val_loss: 0.5006\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4306 - val_loss: 0.4968\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4264 - val_loss: 0.4940\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4224 - val_loss: 0.4901\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4186 - val_loss: 0.4873\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4154 - val_loss: 0.4845\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4118 - val_loss: 0.4828\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4090 - val_loss: 0.4789\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.4061 - val_loss: 0.4768\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4033 - val_loss: 0.4737\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4007 - val_loss: 0.4717\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3983 - val_loss: 0.4704\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3958 - val_loss: 0.4672\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3938 - val_loss: 0.4659\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3916 - val_loss: 0.4638\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3897 - val_loss: 0.4617\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3879 - val_loss: 0.4597\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3861 - val_loss: 0.4586\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3845 - val_loss: 0.4574\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3828 - val_loss: 0.4561\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3814 - val_loss: 0.4547\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3799 - val_loss: 0.4531\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3784 - val_loss: 0.4526\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3773 - val_loss: 0.4506\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3759 - val_loss: 0.4497\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3747 - val_loss: 0.4481\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3735 - val_loss: 0.4475\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3725 - val_loss: 0.4469\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3715 - val_loss: 0.4456\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3705 - val_loss: 0.4442\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3697 - val_loss: 0.4437\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3688 - val_loss: 0.4429\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3677 - val_loss: 0.4417\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3669 - val_loss: 0.4415\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3659 - val_loss: 0.4408\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3653 - val_loss: 0.4407\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3648 - val_loss: 0.4393\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3642 - val_loss: 0.4385\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3633 - val_loss: 0.4381\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3628 - val_loss: 0.4377\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3620 - val_loss: 0.4369\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3614 - val_loss: 0.4367\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3604 - val_loss: 0.4369\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3601 - val_loss: 0.4350\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3594 - val_loss: 0.4349\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3590 - val_loss: 0.4347\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3581 - val_loss: 0.4340\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3577 - val_loss: 0.4333\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3572 - val_loss: 0.4323\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3566 - val_loss: 0.4324\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3561 - val_loss: 0.4319\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3554 - val_loss: 0.4311\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3549 - val_loss: 0.4320\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.3544 - val_loss: 0.4311\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3541 - val_loss: 0.4313\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3535 - val_loss: 0.4305\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3530 - val_loss: 0.4292\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3524 - val_loss: 0.4278\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3521 - val_loss: 0.4277\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.3515 - val_loss: 0.4285\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3511 - val_loss: 0.4275\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3507 - val_loss: 0.4269\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3501 - val_loss: 0.4278\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3499 - val_loss: 0.4270\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3494 - val_loss: 0.4251\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3490 - val_loss: 0.4258\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3486 - val_loss: 0.4252\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3482 - val_loss: 0.4250\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3478 - val_loss: 0.4247\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3472 - val_loss: 0.4245\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3469 - val_loss: 0.4234\n",
      "121/121 [==============================] - 0s 492us/step - loss: 0.3516\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0263 - val_loss: 1.4710\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 1.2078 - val_loss: 1.0356\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.9446 - val_loss: 0.8855\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.8193 - val_loss: 0.8011\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.7427 - val_loss: 0.7439\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.6929 - val_loss: 0.7077\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.6597 - val_loss: 0.6830\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.6362 - val_loss: 0.6652\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.6177 - val_loss: 0.6510\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.6021 - val_loss: 0.6396\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5890 - val_loss: 0.6279\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5769 - val_loss: 0.6178\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.5655 - val_loss: 0.6087\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.5553 - val_loss: 0.6014\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.5452 - val_loss: 0.5918\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5355 - val_loss: 0.5851\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5260 - val_loss: 0.5752\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5174 - val_loss: 0.5701\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5088 - val_loss: 0.5645\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.5005 - val_loss: 0.5588\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4926 - val_loss: 0.5521\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4851 - val_loss: 0.5460\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4777 - val_loss: 0.5432\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4706 - val_loss: 0.5337\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4645 - val_loss: 0.5314\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.4583 - val_loss: 0.5286\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4525 - val_loss: 0.5240\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.4471 - val_loss: 0.5197\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4420 - val_loss: 0.5124\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.4373 - val_loss: 0.5118\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.4327 - val_loss: 0.5105\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4289 - val_loss: 0.5043\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4247 - val_loss: 0.5005\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4209 - val_loss: 0.4960\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4176 - val_loss: 0.4946\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4146 - val_loss: 0.4922\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4116 - val_loss: 0.4890\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4088 - val_loss: 0.4873\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4062 - val_loss: 0.4849\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4037 - val_loss: 0.4830\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.4010 - val_loss: 0.4792\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3993 - val_loss: 0.4793\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3971 - val_loss: 0.4772\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3953 - val_loss: 0.4744\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3937 - val_loss: 0.4731\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3917 - val_loss: 0.4724\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3900 - val_loss: 0.4698\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3884 - val_loss: 0.4686\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3868 - val_loss: 0.4675\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3856 - val_loss: 0.4660\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3841 - val_loss: 0.4646\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3828 - val_loss: 0.4652\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3818 - val_loss: 0.4627\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3806 - val_loss: 0.4599\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3794 - val_loss: 0.4582\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3785 - val_loss: 0.4588\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3774 - val_loss: 0.4595\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3764 - val_loss: 0.4578\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3754 - val_loss: 0.4575\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3745 - val_loss: 0.4553\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3734 - val_loss: 0.4541\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3724 - val_loss: 0.4545\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3717 - val_loss: 0.4535\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3706 - val_loss: 0.4523\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3699 - val_loss: 0.4515\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3688 - val_loss: 0.4510\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3683 - val_loss: 0.4502\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3671 - val_loss: 0.4515\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3666 - val_loss: 0.4493\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3658 - val_loss: 0.4497\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3648 - val_loss: 0.4472\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3642 - val_loss: 0.4465\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3632 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3628 - val_loss: 0.4460\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3618 - val_loss: 0.4461\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3610 - val_loss: 0.4453\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3603 - val_loss: 0.4443\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3599 - val_loss: 0.4443\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3592 - val_loss: 0.4439\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3587 - val_loss: 0.4437\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3578 - val_loss: 0.4433\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3575 - val_loss: 0.4435\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3568 - val_loss: 0.4432\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3558 - val_loss: 0.4412\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3554 - val_loss: 0.4420\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3550 - val_loss: 0.4416\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3544 - val_loss: 0.4391\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3536 - val_loss: 0.4408\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3536 - val_loss: 0.4393\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3523 - val_loss: 0.4396\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3526 - val_loss: 0.4386\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3520 - val_loss: 0.4380\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3512 - val_loss: 0.4378\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3508 - val_loss: 0.4386\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3505 - val_loss: 0.4379\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3500 - val_loss: 0.4373\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3496 - val_loss: 0.4378\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3491 - val_loss: 0.4371\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.3487 - val_loss: 0.4359\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3482 - val_loss: 0.4365\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.3518\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7094 - val_loss: 1.4389\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 1.2534 - val_loss: 1.0057\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.9180 - val_loss: 0.7980\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.7726 - val_loss: 0.7155\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.7078 - val_loss: 0.6751\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.6730 - val_loss: 0.6494\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.6486 - val_loss: 0.6311\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.6289 - val_loss: 0.6157\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.6119 - val_loss: 0.6025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.5970 - val_loss: 0.5909\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5831 - val_loss: 0.5809\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.5705 - val_loss: 0.5717\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5588 - val_loss: 0.5635\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.5478 - val_loss: 0.5560\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5379 - val_loss: 0.5497\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5283 - val_loss: 0.5432\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.5198 - val_loss: 0.5382\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5117 - val_loss: 0.5334\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.5041 - val_loss: 0.5301\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4973 - val_loss: 0.5252\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.4908 - val_loss: 0.5209\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4847 - val_loss: 0.5179\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4792 - val_loss: 0.5149\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.4738 - val_loss: 0.5120\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4685 - val_loss: 0.5103\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4638 - val_loss: 0.5060\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4592 - val_loss: 0.5031\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4551 - val_loss: 0.5009\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4513 - val_loss: 0.4995\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4475 - val_loss: 0.4984\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4440 - val_loss: 0.4947\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4405 - val_loss: 0.4940\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4372 - val_loss: 0.4912\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4341 - val_loss: 0.4906\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4311 - val_loss: 0.4890\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4283 - val_loss: 0.4866\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4255 - val_loss: 0.4850\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4231 - val_loss: 0.4839\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.4205 - val_loss: 0.4810\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4184 - val_loss: 0.4814\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.4160 - val_loss: 0.4795\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4138 - val_loss: 0.4786\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4113 - val_loss: 0.4775\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4095 - val_loss: 0.4752\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.4076 - val_loss: 0.4745\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.4056 - val_loss: 0.4734\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4035 - val_loss: 0.4738\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4016 - val_loss: 0.4724\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3999 - val_loss: 0.4707\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3981 - val_loss: 0.4696\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3963 - val_loss: 0.4693\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3948 - val_loss: 0.4668\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3931 - val_loss: 0.4664\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3915 - val_loss: 0.4648\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3899 - val_loss: 0.4645\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3883 - val_loss: 0.4642\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3869 - val_loss: 0.4623\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3852 - val_loss: 0.4615\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3836 - val_loss: 0.4627\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3824 - val_loss: 0.4605\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3807 - val_loss: 0.4585\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3794 - val_loss: 0.4585\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3781 - val_loss: 0.4571\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3767 - val_loss: 0.4570\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3753 - val_loss: 0.4568\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3739 - val_loss: 0.4551\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3729 - val_loss: 0.4548\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3714 - val_loss: 0.4533\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3704 - val_loss: 0.4535\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3690 - val_loss: 0.4533\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3680 - val_loss: 0.4520\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3668 - val_loss: 0.4520\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3655 - val_loss: 0.4499\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3647 - val_loss: 0.4491\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3635 - val_loss: 0.4502\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3626 - val_loss: 0.4481\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3617 - val_loss: 0.4468\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3608 - val_loss: 0.4474\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3598 - val_loss: 0.4470\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3589 - val_loss: 0.4464\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3580 - val_loss: 0.4460\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3571 - val_loss: 0.4442\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3563 - val_loss: 0.4444\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3555 - val_loss: 0.4433\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3546 - val_loss: 0.4425\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3538 - val_loss: 0.4423\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3531 - val_loss: 0.4407\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3524 - val_loss: 0.4407\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3515 - val_loss: 0.4400\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3507 - val_loss: 0.4395\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3501 - val_loss: 0.4396\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3494 - val_loss: 0.4397\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3489 - val_loss: 0.4388\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3481 - val_loss: 0.4390\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3475 - val_loss: 0.4382\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3467 - val_loss: 0.4371\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3461 - val_loss: 0.4378\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3454 - val_loss: 0.4370\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3450 - val_loss: 0.4359\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3442 - val_loss: 0.4360\n",
      "121/121 [==============================] - 0s 500us/step - loss: 0.3578\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 1.0536 - val_loss: 0.9229\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 1.0595 - val_loss: 0.5371\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4094 - val_loss: 0.5062\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3972 - val_loss: 0.5009\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3908 - val_loss: 0.4829\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3978 - val_loss: 0.5158\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3836 - val_loss: 0.5346\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3784 - val_loss: 0.4990\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3796 - val_loss: 0.5114\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3773 - val_loss: 0.4892\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.3710 - val_loss: 0.4896\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3697 - val_loss: 0.4993\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3671 - val_loss: 0.4813\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3663 - val_loss: 0.4885\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3707 - val_loss: 0.4785\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3655 - val_loss: 0.4814\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3612 - val_loss: 0.4836\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3634 - val_loss: 0.4521\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3599 - val_loss: 0.4785\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3609 - val_loss: 0.4507\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3572 - val_loss: 0.4578\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3565 - val_loss: 0.4710\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3562 - val_loss: 0.4679\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3561 - val_loss: 0.4685\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3542 - val_loss: 0.4902\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3544 - val_loss: 0.4556\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.3502 - val_loss: 0.4783\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3512 - val_loss: 0.4562\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3515 - val_loss: 0.4554\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3493 - val_loss: 0.4562\n",
      "121/121 [==============================] - 0s 485us/step - loss: 0.3538\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.9631 - val_loss: 0.6811\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.5542 - val_loss: 0.5738\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.4766 - val_loss: 0.5254\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4451 - val_loss: 0.4999\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4247 - val_loss: 0.4890\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4126 - val_loss: 0.4843\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.4041 - val_loss: 0.4774\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3980 - val_loss: 0.4719\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3899 - val_loss: 0.4705\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3853 - val_loss: 0.4740\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3830 - val_loss: 0.4555\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3793 - val_loss: 0.4544\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3756 - val_loss: 0.4593\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3742 - val_loss: 0.4541\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3720 - val_loss: 0.4584\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3691 - val_loss: 0.4480\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3693 - val_loss: 0.4512\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3677 - val_loss: 0.4535\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.3664 - val_loss: 0.4531\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3650 - val_loss: 0.4484\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3638 - val_loss: 0.4430\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.3616 - val_loss: 0.4462\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3614 - val_loss: 0.4447\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3605 - val_loss: 0.4549\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3605 - val_loss: 0.4427\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3581 - val_loss: 0.4393\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3570 - val_loss: 0.4409\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3575 - val_loss: 0.4446\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3560 - val_loss: 0.4326\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3546 - val_loss: 0.4438\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3545 - val_loss: 0.4385\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3531 - val_loss: 0.4406\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3532 - val_loss: 0.4373\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3545 - val_loss: 0.4334\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3515 - val_loss: 0.4438\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3525 - val_loss: 0.4387\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3500 - val_loss: 0.4427\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3503 - val_loss: 0.4410\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3495 - val_loss: 0.4344\n",
      "121/121 [==============================] - 0s 462us/step - loss: 0.3517\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0598 - val_loss: 2.5579\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.8131 - val_loss: 0.5728\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.4964 - val_loss: 0.5197\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.4621 - val_loss: 0.4981\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4419 - val_loss: 0.4922\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4262 - val_loss: 0.4862\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4164 - val_loss: 0.4754\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.4093 - val_loss: 0.4676\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4038 - val_loss: 0.4696\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3964 - val_loss: 0.4644\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3916 - val_loss: 0.4594\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3872 - val_loss: 0.4544\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3814 - val_loss: 0.4573\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3786 - val_loss: 0.4562\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3735 - val_loss: 0.4469\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3709 - val_loss: 0.4452\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3717 - val_loss: 0.4471\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3670 - val_loss: 0.4449\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3640 - val_loss: 0.4390\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3612 - val_loss: 0.4404\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3598 - val_loss: 0.4512\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3575 - val_loss: 0.4344\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3581 - val_loss: 0.4454\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3554 - val_loss: 0.4359\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3543 - val_loss: 0.4496\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3538 - val_loss: 0.4501\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3529 - val_loss: 0.4349\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3518 - val_loss: 0.4324\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3455 - val_loss: 0.4347\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3511 - val_loss: 0.4298\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3468 - val_loss: 0.4323\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.3471 - val_loss: 0.4430\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3461 - val_loss: 0.4349\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3417 - val_loss: 0.4449\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3455 - val_loss: 0.4284\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3788 - val_loss: 0.5940\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.3584 - val_loss: 0.4379\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3433 - val_loss: 0.4455\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3421 - val_loss: 0.4208\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3397 - val_loss: 0.4301\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3394 - val_loss: 0.4277\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3393 - val_loss: 0.4220\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3366 - val_loss: 0.4231\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3384 - val_loss: 0.4429\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3390 - val_loss: 0.4264\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3377 - val_loss: 0.4391\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3349 - val_loss: 0.4327\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3358 - val_loss: 0.4226\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3327 - val_loss: 0.4306\n",
      "121/121 [==============================] - 0s 457us/step - loss: 0.3503\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 4.3215 - val_loss: 2.9888\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 2.3087 - val_loss: 1.7920\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 1.4921 - val_loss: 1.2770\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 1.1231 - val_loss: 1.0292\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.9385 - val_loss: 0.8972\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.8378 - val_loss: 0.8202\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.7772 - val_loss: 0.7702\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.7373 - val_loss: 0.7353\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.7085 - val_loss: 0.7091\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.6865 - val_loss: 0.6889\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.6691 - val_loss: 0.6727\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.6547 - val_loss: 0.6595\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.6425 - val_loss: 0.6483\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.6320 - val_loss: 0.6388\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.6228 - val_loss: 0.6307\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.6146 - val_loss: 0.6235\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.6073 - val_loss: 0.6171\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.6007 - val_loss: 0.6116\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.5946 - val_loss: 0.6065\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.5890 - val_loss: 0.6019\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5838 - val_loss: 0.5977\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.5789 - val_loss: 0.5937\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.5743 - val_loss: 0.5900\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5699 - val_loss: 0.5865\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5658 - val_loss: 0.5834\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.5619 - val_loss: 0.5804\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.5581 - val_loss: 0.5775\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.5544 - val_loss: 0.5748\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.5510 - val_loss: 0.5722\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.5477 - val_loss: 0.5699\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.5445 - val_loss: 0.5675\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.5415 - val_loss: 0.5653\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.5385 - val_loss: 0.5632\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.5357 - val_loss: 0.5612\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.5329 - val_loss: 0.5592\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.5303 - val_loss: 0.5573\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.5277 - val_loss: 0.5555\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.5252 - val_loss: 0.5537\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5227 - val_loss: 0.5520\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5203 - val_loss: 0.5503\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5179 - val_loss: 0.5487\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.5157 - val_loss: 0.5471\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.5134 - val_loss: 0.5456\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.5113 - val_loss: 0.5440\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5092 - val_loss: 0.5427\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.5071 - val_loss: 0.5413\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.5050 - val_loss: 0.5399\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.5030 - val_loss: 0.5385\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.5011 - val_loss: 0.5372\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4991 - val_loss: 0.5360\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.4972 - val_loss: 0.5348\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4954 - val_loss: 0.5336\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.4936 - val_loss: 0.5324\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4918 - val_loss: 0.5312\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.4900 - val_loss: 0.5301\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4883 - val_loss: 0.5291\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4866 - val_loss: 0.5280\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.4849 - val_loss: 0.5270\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4833 - val_loss: 0.5260\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4817 - val_loss: 0.5249\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4802 - val_loss: 0.5239\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4786 - val_loss: 0.5230\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4771 - val_loss: 0.5221\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4756 - val_loss: 0.5211\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4742 - val_loss: 0.5202\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4728 - val_loss: 0.5192\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4715 - val_loss: 0.5183\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4701 - val_loss: 0.5173\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4688 - val_loss: 0.5166\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.4675 - val_loss: 0.5158\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.4662 - val_loss: 0.5150\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.4650 - val_loss: 0.5143\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.4638 - val_loss: 0.5134\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4625 - val_loss: 0.5126\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4613 - val_loss: 0.5119\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4601 - val_loss: 0.5111\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4589 - val_loss: 0.5104\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4578 - val_loss: 0.5097\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4567 - val_loss: 0.5090\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4555 - val_loss: 0.5082\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4544 - val_loss: 0.5074\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4534 - val_loss: 0.5068\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4523 - val_loss: 0.5060\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4513 - val_loss: 0.5054\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4502 - val_loss: 0.5048\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.4492 - val_loss: 0.5042\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.4482 - val_loss: 0.5036\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4472 - val_loss: 0.5030\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.4462 - val_loss: 0.5024\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4453 - val_loss: 0.5018\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4443 - val_loss: 0.5011\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4435 - val_loss: 0.5005\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.4426 - val_loss: 0.4999\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4417 - val_loss: 0.4994\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4408 - val_loss: 0.4989\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.4400 - val_loss: 0.4983\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4391 - val_loss: 0.4977\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4383 - val_loss: 0.4972\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4374 - val_loss: 0.4967\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4366 - val_loss: 0.4961\n",
      "121/121 [==============================] - 0s 515us/step - loss: 0.4484\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.2466 - val_loss: 3.6171\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 2.8600 - val_loss: 2.2017\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 1.8834 - val_loss: 1.5714\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 1.4100 - val_loss: 1.2389\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 1.1497 - val_loss: 1.0472\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.9932 - val_loss: 0.9282\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.8934 - val_loss: 0.8507\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.8273 - val_loss: 0.7981\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.7816 - val_loss: 0.7612\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.7489 - val_loss: 0.7345\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.7245 - val_loss: 0.7146\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.7056 - val_loss: 0.6989\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.6903 - val_loss: 0.6863\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.6774 - val_loss: 0.6756\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.6662 - val_loss: 0.6664\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.6562 - val_loss: 0.6582\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.6472 - val_loss: 0.6509\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.6389 - val_loss: 0.6443\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.6312 - val_loss: 0.6382\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.6241 - val_loss: 0.6325\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.6174 - val_loss: 0.6273\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.6111 - val_loss: 0.6224\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.6051 - val_loss: 0.6178\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.5994 - val_loss: 0.6135\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.5940 - val_loss: 0.6094\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5888 - val_loss: 0.6055\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5838 - val_loss: 0.6018\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5790 - val_loss: 0.5982\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5744 - val_loss: 0.5948\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.5699 - val_loss: 0.5916\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.5656 - val_loss: 0.5885\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.5615 - val_loss: 0.5855\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.5575 - val_loss: 0.5826\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.5536 - val_loss: 0.5799\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.5499 - val_loss: 0.5772\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5462 - val_loss: 0.5747\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.5427 - val_loss: 0.5723\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.5393 - val_loss: 0.5699\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.5360 - val_loss: 0.5676\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.5328 - val_loss: 0.5654\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5297 - val_loss: 0.5632\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.5266 - val_loss: 0.5611\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.5236 - val_loss: 0.5591\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.5208 - val_loss: 0.5572\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5179 - val_loss: 0.5553\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.5152 - val_loss: 0.5535\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.5125 - val_loss: 0.5517\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.5099 - val_loss: 0.5499\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.5074 - val_loss: 0.5483\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.5049 - val_loss: 0.5466\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5024 - val_loss: 0.5451\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5000 - val_loss: 0.5436\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.4977 - val_loss: 0.5421\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.4954 - val_loss: 0.5407\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.4932 - val_loss: 0.5393\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.4911 - val_loss: 0.5379\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4889 - val_loss: 0.5366\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.4869 - val_loss: 0.5353\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4848 - val_loss: 0.5340\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4829 - val_loss: 0.5327\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4809 - val_loss: 0.5316\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4790 - val_loss: 0.5304\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4771 - val_loss: 0.5292\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4753 - val_loss: 0.5281\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4735 - val_loss: 0.5270\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4717 - val_loss: 0.5259\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.4700 - val_loss: 0.5248\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.4684 - val_loss: 0.5238\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4667 - val_loss: 0.5228\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4651 - val_loss: 0.5216\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.4635 - val_loss: 0.5206\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4620 - val_loss: 0.5196\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4605 - val_loss: 0.5186\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4590 - val_loss: 0.5176\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4575 - val_loss: 0.5167\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4561 - val_loss: 0.5158\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4547 - val_loss: 0.5149\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.4533 - val_loss: 0.5140\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4519 - val_loss: 0.5131\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.4506 - val_loss: 0.5122\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4493 - val_loss: 0.5113\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4480 - val_loss: 0.5105\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4468 - val_loss: 0.5098\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.4455 - val_loss: 0.5089\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4444 - val_loss: 0.5081\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4432 - val_loss: 0.5073\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.4421 - val_loss: 0.5065\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4409 - val_loss: 0.5058\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4399 - val_loss: 0.5052\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4388 - val_loss: 0.5044\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.4377 - val_loss: 0.5038\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4368 - val_loss: 0.5032\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4358 - val_loss: 0.5026\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4348 - val_loss: 0.5020\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4338 - val_loss: 0.5012\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4329 - val_loss: 0.5006\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4319 - val_loss: 0.5000\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4310 - val_loss: 0.4994\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4301 - val_loss: 0.4988\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4292 - val_loss: 0.4982\n",
      "121/121 [==============================] - 0s 488us/step - loss: 0.4454\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6350 - val_loss: 2.9716\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 2.3561 - val_loss: 1.7990\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 1.6006 - val_loss: 1.3376\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 1.2709 - val_loss: 1.1120\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 1.0954 - val_loss: 0.9857\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.9903 - val_loss: 0.9084\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.9218 - val_loss: 0.8579\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.8747 - val_loss: 0.8238\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.8413 - val_loss: 0.7994\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.8160 - val_loss: 0.7810\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.7963 - val_loss: 0.7667\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.7800 - val_loss: 0.7549\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.7660 - val_loss: 0.7448\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.7536 - val_loss: 0.7356\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.7424 - val_loss: 0.7273\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.7319 - val_loss: 0.7197\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.7220 - val_loss: 0.7124\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.7126 - val_loss: 0.7054\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.7038 - val_loss: 0.6988\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.6953 - val_loss: 0.6922\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.6871 - val_loss: 0.6861\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6791 - val_loss: 0.6800\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.6714 - val_loss: 0.6743\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6639 - val_loss: 0.6686\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.6567 - val_loss: 0.6634\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.6498 - val_loss: 0.6581\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.6431 - val_loss: 0.6531\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.6365 - val_loss: 0.6485\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.6303 - val_loss: 0.6438\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.6242 - val_loss: 0.6394\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.6184 - val_loss: 0.6350\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.6127 - val_loss: 0.6309\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.6073 - val_loss: 0.6269\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.6021 - val_loss: 0.6232\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.5969 - val_loss: 0.6195\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5920 - val_loss: 0.6162\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5873 - val_loss: 0.6126\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5827 - val_loss: 0.6093\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.5783 - val_loss: 0.6062\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.5740 - val_loss: 0.6032\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5699 - val_loss: 0.6005\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5659 - val_loss: 0.5976\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.5620 - val_loss: 0.5949\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.5582 - val_loss: 0.5923\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.5545 - val_loss: 0.5897\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.5509 - val_loss: 0.5872\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.5475 - val_loss: 0.5848\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.5441 - val_loss: 0.5826\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.5409 - val_loss: 0.5804\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.5378 - val_loss: 0.5783\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5348 - val_loss: 0.5761\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.5320 - val_loss: 0.5743\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.5291 - val_loss: 0.5724\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.5264 - val_loss: 0.5705\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5237 - val_loss: 0.5688\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.5212 - val_loss: 0.5670\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5186 - val_loss: 0.5652\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.5162 - val_loss: 0.5637\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.5138 - val_loss: 0.5621\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.5115 - val_loss: 0.5604\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.5093 - val_loss: 0.5588\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.5070 - val_loss: 0.5575\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.5049 - val_loss: 0.5559\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.5028 - val_loss: 0.5544\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.5008 - val_loss: 0.5530\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.4988 - val_loss: 0.5517\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4968 - val_loss: 0.5505\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4950 - val_loss: 0.5491\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4931 - val_loss: 0.5477\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4913 - val_loss: 0.5465\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.4894 - val_loss: 0.5452\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4877 - val_loss: 0.5440\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4860 - val_loss: 0.5430\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4843 - val_loss: 0.5418\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4827 - val_loss: 0.5405\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4811 - val_loss: 0.5397\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4795 - val_loss: 0.5386\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.4780 - val_loss: 0.5376\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4765 - val_loss: 0.5366\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4750 - val_loss: 0.5355\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4735 - val_loss: 0.5346\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4721 - val_loss: 0.5335\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.4708 - val_loss: 0.5325\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4694 - val_loss: 0.5317\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.4681 - val_loss: 0.5309\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4667 - val_loss: 0.5300\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.4654 - val_loss: 0.5291\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4641 - val_loss: 0.5286\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.4629 - val_loss: 0.5278\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.4617 - val_loss: 0.5270\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4605 - val_loss: 0.5261\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4593 - val_loss: 0.5252\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4581 - val_loss: 0.5246\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4570 - val_loss: 0.5239\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4558 - val_loss: 0.5231\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.4547 - val_loss: 0.5223\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4537 - val_loss: 0.5215\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4526 - val_loss: 0.5207\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.4515 - val_loss: 0.5201\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4505 - val_loss: 0.5193\n",
      "121/121 [==============================] - 0s 493us/step - loss: 0.4404\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1339 - val_loss: 1.3491\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.8211 - val_loss: 5.9386\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 18.2151 - val_loss: 76.8836\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 131.7829 - val_loss: 966.2897\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 2605.7415 - val_loss: 12935.4717\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 29588.7109 - val_loss: 170958.0938\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 92546.6484 - val_loss: 2311258.7500\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 1321135.5000 - val_loss: 29933994.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 19748406.0000 - val_loss: 395945088.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 427968160.0000 - val_loss: 5184958464.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 12247938048.0000 - val_loss: 68786208768.0000\n",
      "121/121 [==============================] - 0s 507us/step - loss: 35004293120.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 1.0356 - val_loss: 0.5790\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.5186 - val_loss: 0.5415\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5126 - val_loss: 0.5457\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5167 - val_loss: 0.5460\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5193 - val_loss: 0.5431\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5156 - val_loss: 0.5392\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5154 - val_loss: 0.5400\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5138 - val_loss: 0.5360\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5167 - val_loss: 0.5533\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.5151 - val_loss: 0.5507\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5082 - val_loss: 0.5485\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5238 - val_loss: 0.5501\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5122 - val_loss: 0.5537\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5171 - val_loss: 0.5431\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5132 - val_loss: 0.5454\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.5212 - val_loss: 0.5737\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.5166 - val_loss: 0.5403\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5129 - val_loss: 0.5507\n",
      "121/121 [==============================] - 0s 457us/step - loss: 0.5418\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 5.5106 - val_loss: 1.4784\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.9094 - val_loss: 3.5146\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 1.5360 - val_loss: 10.5831\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 48.4615 - val_loss: 24.0685\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 206.1616 - val_loss: 55.5226\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 32.0229 - val_loss: 143.2668\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 1141.1167 - val_loss: 330.2784\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 1049.8733 - val_loss: 7974.7930\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 41238.0469 - val_loss: 22292.1191\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 7220.5273 - val_loss: 52161.0352\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 62015.4570 - val_loss: 1290648.6250\n",
      "121/121 [==============================] - 0s 447us/step - loss: 78058.6250\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3332 - val_loss: 1.3515\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 1.1665 - val_loss: 0.8597\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.7274 - val_loss: 0.6683\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.6130 - val_loss: 0.6052\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.5633 - val_loss: 0.5769\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.5300 - val_loss: 0.5563\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5032 - val_loss: 0.5379\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4823 - val_loss: 0.5243\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.4642 - val_loss: 0.5117\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4493 - val_loss: 0.5018\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4384 - val_loss: 0.4931\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4282 - val_loss: 0.4859\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4201 - val_loss: 0.4817\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.4133 - val_loss: 0.4767\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4075 - val_loss: 0.4716\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4023 - val_loss: 0.4665\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3977 - val_loss: 0.4633\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3935 - val_loss: 0.4597\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3894 - val_loss: 0.4579\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3860 - val_loss: 0.4564\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3831 - val_loss: 0.4524\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3800 - val_loss: 0.4496\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3768 - val_loss: 0.4484\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3745 - val_loss: 0.4452\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3720 - val_loss: 0.4429\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3694 - val_loss: 0.4435\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3678 - val_loss: 0.4405\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3658 - val_loss: 0.4396\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3638 - val_loss: 0.4386\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3620 - val_loss: 0.4382\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3604 - val_loss: 0.4352\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3588 - val_loss: 0.4347\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3573 - val_loss: 0.4344\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3557 - val_loss: 0.4325\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3540 - val_loss: 0.4333\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3533 - val_loss: 0.4299\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3521 - val_loss: 0.4319\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3508 - val_loss: 0.4309\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3498 - val_loss: 0.4277\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3483 - val_loss: 0.4307\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3470 - val_loss: 0.4306\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3461 - val_loss: 0.4263\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3456 - val_loss: 0.4284\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3446 - val_loss: 0.4256\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3438 - val_loss: 0.4254\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3424 - val_loss: 0.4259\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3419 - val_loss: 0.4252\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3415 - val_loss: 0.4211\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3402 - val_loss: 0.4233\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3396 - val_loss: 0.4218\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3390 - val_loss: 0.4233\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3385 - val_loss: 0.4230\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3382 - val_loss: 0.4223\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3372 - val_loss: 0.4239\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3360 - val_loss: 0.4218\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3348 - val_loss: 0.4251\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3343 - val_loss: 0.4203\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3336 - val_loss: 0.4175\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3326 - val_loss: 0.4221\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3322 - val_loss: 0.4177\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3318 - val_loss: 0.4208\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3311 - val_loss: 0.4191\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3303 - val_loss: 0.4172\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3297 - val_loss: 0.4198\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3290 - val_loss: 0.4151\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3291 - val_loss: 0.4168\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3281 - val_loss: 0.4213\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3273 - val_loss: 0.4200\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3270 - val_loss: 0.4153\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3262 - val_loss: 0.4132\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3255 - val_loss: 0.4148\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3255 - val_loss: 0.4138\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3249 - val_loss: 0.4183\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3241 - val_loss: 0.4215\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3239 - val_loss: 0.4156\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3226 - val_loss: 0.4172\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3226 - val_loss: 0.4146\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3223 - val_loss: 0.4110\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3219 - val_loss: 0.4119\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3212 - val_loss: 0.4149\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3210 - val_loss: 0.4152\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3204 - val_loss: 0.4131\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3206 - val_loss: 0.4085\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3197 - val_loss: 0.4106\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3189 - val_loss: 0.4102\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3183 - val_loss: 0.4126\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3183 - val_loss: 0.4082\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3176 - val_loss: 0.4146\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3171 - val_loss: 0.4128\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3168 - val_loss: 0.4173\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3164 - val_loss: 0.4128\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3159 - val_loss: 0.4084\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3154 - val_loss: 0.4145\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3155 - val_loss: 0.4142\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3148 - val_loss: 0.4089\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3141 - val_loss: 0.4140\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3137 - val_loss: 0.4079\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3136 - val_loss: 0.4124\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3131 - val_loss: 0.4102\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3124 - val_loss: 0.4036\n",
      "121/121 [==============================] - 0s 526us/step - loss: 0.3234\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0727 - val_loss: 0.9866\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.7555 - val_loss: 0.7330\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.6406 - val_loss: 0.6540\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5916 - val_loss: 0.6100\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.5559 - val_loss: 0.5763\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.5273 - val_loss: 0.5527\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.5047 - val_loss: 0.5366\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4859 - val_loss: 0.5227\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4698 - val_loss: 0.5124\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4567 - val_loss: 0.5010\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.4451 - val_loss: 0.4934\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4349 - val_loss: 0.4898\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4267 - val_loss: 0.4827\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4194 - val_loss: 0.4802\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4133 - val_loss: 0.4740\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.4076 - val_loss: 0.4720\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4026 - val_loss: 0.4687\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3983 - val_loss: 0.4643\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3947 - val_loss: 0.4633\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3909 - val_loss: 0.4583\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3881 - val_loss: 0.4600\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3849 - val_loss: 0.4555\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3826 - val_loss: 0.4522\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3799 - val_loss: 0.4522\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3778 - val_loss: 0.4537\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3755 - val_loss: 0.4480\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3739 - val_loss: 0.4478\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3717 - val_loss: 0.4455\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3696 - val_loss: 0.4477\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3680 - val_loss: 0.4446\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3663 - val_loss: 0.4437\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3647 - val_loss: 0.4427\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3633 - val_loss: 0.4435\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3618 - val_loss: 0.4402\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3602 - val_loss: 0.4381\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3590 - val_loss: 0.4374\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3575 - val_loss: 0.4366\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3564 - val_loss: 0.4369\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3551 - val_loss: 0.4367\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3538 - val_loss: 0.4322\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3527 - val_loss: 0.4336\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3515 - val_loss: 0.4333\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3504 - val_loss: 0.4326\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3494 - val_loss: 0.4318\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3480 - val_loss: 0.4307\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3474 - val_loss: 0.4339\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3464 - val_loss: 0.4297\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3454 - val_loss: 0.4288\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3444 - val_loss: 0.4293\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3438 - val_loss: 0.4298\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3425 - val_loss: 0.4309\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3418 - val_loss: 0.4283\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3412 - val_loss: 0.4270\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3404 - val_loss: 0.4276\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3399 - val_loss: 0.4264\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3387 - val_loss: 0.4259\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3379 - val_loss: 0.4240\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3375 - val_loss: 0.4255\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3368 - val_loss: 0.4237\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3361 - val_loss: 0.4238\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3356 - val_loss: 0.4256\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3346 - val_loss: 0.4218\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3342 - val_loss: 0.4244\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3335 - val_loss: 0.4234\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3328 - val_loss: 0.4226\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3321 - val_loss: 0.4204\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3316 - val_loss: 0.4217\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3310 - val_loss: 0.4212\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3302 - val_loss: 0.4198\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3294 - val_loss: 0.4240\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3292 - val_loss: 0.4189\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3284 - val_loss: 0.4188\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3283 - val_loss: 0.4198\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3272 - val_loss: 0.4198\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3269 - val_loss: 0.4190\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3265 - val_loss: 0.4191\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3260 - val_loss: 0.4193\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3252 - val_loss: 0.4189\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3245 - val_loss: 0.4165\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3236 - val_loss: 0.4179\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3237 - val_loss: 0.4176\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3230 - val_loss: 0.4147\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3225 - val_loss: 0.4169\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3219 - val_loss: 0.4178\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3216 - val_loss: 0.4165\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3210 - val_loss: 0.4172\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3204 - val_loss: 0.4151\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3201 - val_loss: 0.4140\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3196 - val_loss: 0.4146\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3189 - val_loss: 0.4174\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3181 - val_loss: 0.4158\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3181 - val_loss: 0.4128\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3177 - val_loss: 0.4160\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3171 - val_loss: 0.4118\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3164 - val_loss: 0.4130\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3159 - val_loss: 0.4122\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3157 - val_loss: 0.4120\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3155 - val_loss: 0.4144\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3152 - val_loss: 0.4116\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3145 - val_loss: 0.4148\n",
      "121/121 [==============================] - 0s 492us/step - loss: 0.3251\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0480 - val_loss: 0.8981\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.8024 - val_loss: 0.6764\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.6603 - val_loss: 0.6302\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.6147 - val_loss: 0.6034\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5810 - val_loss: 0.5821\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.5532 - val_loss: 0.5653\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.5309 - val_loss: 0.5526\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5111 - val_loss: 0.5399\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4942 - val_loss: 0.5289\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4802 - val_loss: 0.5219\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4685 - val_loss: 0.5143\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4573 - val_loss: 0.5052\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4483 - val_loss: 0.5014\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4394 - val_loss: 0.4971\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4314 - val_loss: 0.4884\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4248 - val_loss: 0.4859\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4189 - val_loss: 0.4809\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4130 - val_loss: 0.4802\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4083 - val_loss: 0.4756\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.4040 - val_loss: 0.4714\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3995 - val_loss: 0.4714\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3959 - val_loss: 0.4659\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.3919 - val_loss: 0.4642\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3884 - val_loss: 0.4661\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3858 - val_loss: 0.4593\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3829 - val_loss: 0.4573\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3798 - val_loss: 0.4598\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3775 - val_loss: 0.4543\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3746 - val_loss: 0.4544\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3725 - val_loss: 0.4500\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3706 - val_loss: 0.4506\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3683 - val_loss: 0.4486\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3660 - val_loss: 0.4474\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3645 - val_loss: 0.4477\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3622 - val_loss: 0.4458\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3607 - val_loss: 0.4454\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3589 - val_loss: 0.4442\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3570 - val_loss: 0.4422\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3558 - val_loss: 0.4436\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3543 - val_loss: 0.4416\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3529 - val_loss: 0.4401\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3514 - val_loss: 0.4392\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3499 - val_loss: 0.4399\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3490 - val_loss: 0.4398\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3475 - val_loss: 0.4374\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3459 - val_loss: 0.4374\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3451 - val_loss: 0.4363\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3435 - val_loss: 0.4357\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3428 - val_loss: 0.4338\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3416 - val_loss: 0.4330\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3406 - val_loss: 0.4332\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3392 - val_loss: 0.4319\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3385 - val_loss: 0.4325\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3369 - val_loss: 0.4323\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3367 - val_loss: 0.4310\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3356 - val_loss: 0.4297\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3343 - val_loss: 0.4315\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3330 - val_loss: 0.4276\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3325 - val_loss: 0.4286\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3319 - val_loss: 0.4277\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3312 - val_loss: 0.4268\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3301 - val_loss: 0.4266\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3291 - val_loss: 0.4286\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3283 - val_loss: 0.4278\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3280 - val_loss: 0.4258\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3270 - val_loss: 0.4253\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3263 - val_loss: 0.4236\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3250 - val_loss: 0.4219\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3248 - val_loss: 0.4238\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3240 - val_loss: 0.4219\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3235 - val_loss: 0.4207\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3223 - val_loss: 0.4206\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3219 - val_loss: 0.4209\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3212 - val_loss: 0.4202\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3201 - val_loss: 0.4191\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3197 - val_loss: 0.4195\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3192 - val_loss: 0.4197\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3182 - val_loss: 0.4167\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3177 - val_loss: 0.4173\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3169 - val_loss: 0.4185\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.3170 - val_loss: 0.4202\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3161 - val_loss: 0.4138\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3155 - val_loss: 0.4157\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3146 - val_loss: 0.4147\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3145 - val_loss: 0.4133\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3138 - val_loss: 0.4144\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3131 - val_loss: 0.4133\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3126 - val_loss: 0.4108\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3120 - val_loss: 0.4126\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3113 - val_loss: 0.4134\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3110 - val_loss: 0.4140\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3101 - val_loss: 0.4127\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3100 - val_loss: 0.4102\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3093 - val_loss: 0.4144\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3089 - val_loss: 0.4158\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3084 - val_loss: 0.4112\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3079 - val_loss: 0.4146\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.3073 - val_loss: 0.4129\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3067 - val_loss: 0.4099\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3060 - val_loss: 0.4062\n",
      "121/121 [==============================] - 0s 465us/step - loss: 0.3349\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.3172 - val_loss: 0.9087\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.7584 - val_loss: 0.7249\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.7587 - val_loss: 0.9314\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.7386 - val_loss: 0.8262\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.9176 - val_loss: 1.4434\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 1.2603 - val_loss: 1.7320\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 2.4540 - val_loss: 3.9422\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 3.9518 - val_loss: 6.1801\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 8.9032 - val_loss: 12.5036\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 15.2818 - val_loss: 22.2610\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 29.0403 - val_loss: 46.2522\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 59.2066 - val_loss: 86.3875\n",
      "121/121 [==============================] - 0s 470us/step - loss: 44.1138\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 2.0833 - val_loss: 0.7905\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.7001 - val_loss: 0.6708\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.6279 - val_loss: 0.6362\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5921 - val_loss: 0.6120\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5680 - val_loss: 0.5962\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5506 - val_loss: 0.5800\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.5399 - val_loss: 0.5678\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.5316 - val_loss: 0.5644\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5258 - val_loss: 0.5597\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.5208 - val_loss: 0.5626\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5188 - val_loss: 0.5535\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.5168 - val_loss: 0.5523\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.5150 - val_loss: 0.5525\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.5133 - val_loss: 0.5454\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.5134 - val_loss: 0.5485\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5113 - val_loss: 0.5419\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5124 - val_loss: 0.5431\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5113 - val_loss: 0.5460\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5107 - val_loss: 0.5425\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5111 - val_loss: 0.5435\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5099 - val_loss: 0.5438\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5113 - val_loss: 0.5422\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.5101 - val_loss: 0.5406\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.5104 - val_loss: 0.5416\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5104 - val_loss: 0.5391\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5097 - val_loss: 0.5405\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.5106 - val_loss: 0.5363\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.5107 - val_loss: 0.5379\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5094 - val_loss: 0.5362\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5105 - val_loss: 0.5394\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5095 - val_loss: 0.5388\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.5101 - val_loss: 0.5363\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5098 - val_loss: 0.5374\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.5103 - val_loss: 0.5400\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.5095 - val_loss: 0.5406\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5102 - val_loss: 0.5401\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5101 - val_loss: 0.5356\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5101 - val_loss: 0.5363\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5100 - val_loss: 0.5415\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5091 - val_loss: 0.5386\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5097 - val_loss: 0.5391\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5107 - val_loss: 0.5389\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.5103 - val_loss: 0.5400\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.5094 - val_loss: 0.5379\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5101 - val_loss: 0.5382\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5100 - val_loss: 0.5375\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5097 - val_loss: 0.5357\n",
      "121/121 [==============================] - 0s 462us/step - loss: 0.5383\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 2.1671 - val_loss: 0.6945\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.7338 - val_loss: 0.6380\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5999 - val_loss: 0.6060\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5754 - val_loss: 0.5974\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5594 - val_loss: 0.5865\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.5436 - val_loss: 0.5584\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5502 - val_loss: 0.6182\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.5442 - val_loss: 0.5549\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.5559 - val_loss: 0.5612\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.5276 - val_loss: 0.5493\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.5380 - val_loss: 0.5833\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.5323 - val_loss: 0.5459\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.5274 - val_loss: 0.5446\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5325 - val_loss: 0.5468\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5301 - val_loss: 0.5508\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5265 - val_loss: 0.5452\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.5270 - val_loss: 0.5711\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.5289 - val_loss: 0.5385\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5368 - val_loss: 0.5610\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.5237 - val_loss: 0.5417\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5327 - val_loss: 0.5751\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.5316 - val_loss: 0.5427\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.5498 - val_loss: 0.5461\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5269 - val_loss: 0.5577\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.5278 - val_loss: 0.5582\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5245 - val_loss: 0.5443\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5365 - val_loss: 0.5535\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.5213 - val_loss: 0.5380\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.5520 - val_loss: 0.5532\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5216 - val_loss: 0.5402\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5329 - val_loss: 0.5846\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5306 - val_loss: 0.5498\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5227 - val_loss: 0.5377\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5349 - val_loss: 0.5498\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5252 - val_loss: 0.5386\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5270 - val_loss: 0.5604\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5237 - val_loss: 0.5380\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.5313 - val_loss: 0.5676\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5299 - val_loss: 0.5400\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5664 - val_loss: 0.5455\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5243 - val_loss: 0.5414\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5292 - val_loss: 0.5873\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5311 - val_loss: 0.5418\n",
      "121/121 [==============================] - 0s 493us/step - loss: 0.4986\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1517 - val_loss: 0.9675\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.8550 - val_loss: 0.7972\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.7471 - val_loss: 0.7312\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.6844 - val_loss: 0.6861\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.6360 - val_loss: 0.6509\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.5976 - val_loss: 0.6213\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.5664 - val_loss: 0.6008\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.5410 - val_loss: 0.5824\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.5206 - val_loss: 0.5677\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.5037 - val_loss: 0.5555\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.4891 - val_loss: 0.5448\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4774 - val_loss: 0.5373\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4669 - val_loss: 0.5292\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4576 - val_loss: 0.5221\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4492 - val_loss: 0.5159\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.4414 - val_loss: 0.5100\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.4340 - val_loss: 0.5043\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.4274 - val_loss: 0.4984\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4209 - val_loss: 0.4935\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4153 - val_loss: 0.4882\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4093 - val_loss: 0.4839\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4039 - val_loss: 0.4794\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3991 - val_loss: 0.4758\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3944 - val_loss: 0.4716\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.3902 - val_loss: 0.4684\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3861 - val_loss: 0.4630\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3830 - val_loss: 0.4625\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3797 - val_loss: 0.4596\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3767 - val_loss: 0.4595\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3740 - val_loss: 0.4560\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3714 - val_loss: 0.4548\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3693 - val_loss: 0.4497\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3668 - val_loss: 0.4460\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3648 - val_loss: 0.4473\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3628 - val_loss: 0.4468\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3610 - val_loss: 0.4444\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3593 - val_loss: 0.4457\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.3578 - val_loss: 0.4414\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3562 - val_loss: 0.4380\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3548 - val_loss: 0.4392\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3531 - val_loss: 0.4375\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3523 - val_loss: 0.4375\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3508 - val_loss: 0.4350\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3497 - val_loss: 0.4363\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3485 - val_loss: 0.4361\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3473 - val_loss: 0.4346\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3462 - val_loss: 0.4349\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3454 - val_loss: 0.4294\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3439 - val_loss: 0.4315\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3431 - val_loss: 0.4288\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3419 - val_loss: 0.4296\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3409 - val_loss: 0.4280\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3401 - val_loss: 0.4255\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3392 - val_loss: 0.4262\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3382 - val_loss: 0.4259\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3372 - val_loss: 0.4260\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3366 - val_loss: 0.4248\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3355 - val_loss: 0.4246\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3349 - val_loss: 0.4210\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3333 - val_loss: 0.4248\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3333 - val_loss: 0.4190\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3324 - val_loss: 0.4205\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3314 - val_loss: 0.4185\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3307 - val_loss: 0.4179\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3298 - val_loss: 0.4163\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3291 - val_loss: 0.4161\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3283 - val_loss: 0.4161\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3272 - val_loss: 0.4177\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.3267 - val_loss: 0.4145\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3261 - val_loss: 0.4141\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3254 - val_loss: 0.4137\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3251 - val_loss: 0.4139\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3241 - val_loss: 0.4146\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3238 - val_loss: 0.4118\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3227 - val_loss: 0.4099\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3224 - val_loss: 0.4094\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3214 - val_loss: 0.4121\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.3212 - val_loss: 0.4131\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3207 - val_loss: 0.4107\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3201 - val_loss: 0.4094\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3197 - val_loss: 0.4072\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3189 - val_loss: 0.4067\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3185 - val_loss: 0.4094\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3178 - val_loss: 0.4076\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3175 - val_loss: 0.4066\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3172 - val_loss: 0.4071\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.3163 - val_loss: 0.4056\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3158 - val_loss: 0.4045\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3155 - val_loss: 0.4055\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3148 - val_loss: 0.4046\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3143 - val_loss: 0.4073\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3137 - val_loss: 0.4021\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3132 - val_loss: 0.4062\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3129 - val_loss: 0.4033\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3123 - val_loss: 0.4018\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3120 - val_loss: 0.4025\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3116 - val_loss: 0.4003\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3109 - val_loss: 0.4006\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3104 - val_loss: 0.4002\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3097 - val_loss: 0.4019\n",
      "121/121 [==============================] - 0s 540us/step - loss: 0.3170\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1410 - val_loss: 1.2578\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 1.0297 - val_loss: 0.8511\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.7573 - val_loss: 0.6989\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.6358 - val_loss: 0.6287\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5679 - val_loss: 0.5860\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.5245 - val_loss: 0.5629\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.4944 - val_loss: 0.5403\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4726 - val_loss: 0.5253\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4569 - val_loss: 0.5183\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.4439 - val_loss: 0.5064\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4337 - val_loss: 0.5017\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4251 - val_loss: 0.4965\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4185 - val_loss: 0.4897\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.4119 - val_loss: 0.4828\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.4067 - val_loss: 0.4808\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4019 - val_loss: 0.4772\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3978 - val_loss: 0.4731\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3939 - val_loss: 0.4699\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3905 - val_loss: 0.4712\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3876 - val_loss: 0.4661\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3839 - val_loss: 0.4624\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3812 - val_loss: 0.4619\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3786 - val_loss: 0.4593\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3759 - val_loss: 0.4590\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3740 - val_loss: 0.4538\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3714 - val_loss: 0.4536\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3688 - val_loss: 0.4538\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3670 - val_loss: 0.4493\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3650 - val_loss: 0.4491\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3636 - val_loss: 0.4473\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3613 - val_loss: 0.4465\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3597 - val_loss: 0.4471\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3583 - val_loss: 0.4432\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3572 - val_loss: 0.4423\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3556 - val_loss: 0.4412\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3536 - val_loss: 0.4429\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3527 - val_loss: 0.4405\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3517 - val_loss: 0.4396\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3500 - val_loss: 0.4386\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3485 - val_loss: 0.4402\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3472 - val_loss: 0.4374\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3463 - val_loss: 0.4437\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3460 - val_loss: 0.4362\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.3447 - val_loss: 0.4368\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3433 - val_loss: 0.4327\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3425 - val_loss: 0.4338\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3421 - val_loss: 0.4322\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3413 - val_loss: 0.4333\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3403 - val_loss: 0.4347\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3392 - val_loss: 0.4309\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3383 - val_loss: 0.4333\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3385 - val_loss: 0.4291\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3373 - val_loss: 0.4291\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3368 - val_loss: 0.4287\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3358 - val_loss: 0.4290\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3353 - val_loss: 0.4326\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3346 - val_loss: 0.4298\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3341 - val_loss: 0.4279\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.3340 - val_loss: 0.4313\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3327 - val_loss: 0.4299\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3322 - val_loss: 0.4251\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3320 - val_loss: 0.4289\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3315 - val_loss: 0.4268\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3308 - val_loss: 0.4293\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3303 - val_loss: 0.4262\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3294 - val_loss: 0.4266\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3289 - val_loss: 0.4250\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3288 - val_loss: 0.4252\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3282 - val_loss: 0.4251\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3281 - val_loss: 0.4283\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3271 - val_loss: 0.4228\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3270 - val_loss: 0.4251\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3269 - val_loss: 0.4253\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3260 - val_loss: 0.4259\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.3258 - val_loss: 0.4251\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3252 - val_loss: 0.4264\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3250 - val_loss: 0.4241\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3247 - val_loss: 0.4212\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3246 - val_loss: 0.4216\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3238 - val_loss: 0.4262\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3235 - val_loss: 0.4272\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3230 - val_loss: 0.4212\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3227 - val_loss: 0.4179\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3224 - val_loss: 0.4232\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3218 - val_loss: 0.4218\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.3216 - val_loss: 0.4170\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3211 - val_loss: 0.4254\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3210 - val_loss: 0.4201\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3207 - val_loss: 0.4231\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3202 - val_loss: 0.4227\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3199 - val_loss: 0.4202\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3197 - val_loss: 0.4220\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3190 - val_loss: 0.4174\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.3190 - val_loss: 0.4200\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3189 - val_loss: 0.4165\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3183 - val_loss: 0.4181\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.3180 - val_loss: 0.4211\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3179 - val_loss: 0.4212\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3175 - val_loss: 0.4260\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3172 - val_loss: 0.4272\n",
      "121/121 [==============================] - 0s 496us/step - loss: 0.3264\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4672 - val_loss: 0.8552\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.7718 - val_loss: 0.6816\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.6669 - val_loss: 0.6464\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6289 - val_loss: 0.6293\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.6035 - val_loss: 0.6113\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.5814 - val_loss: 0.5964\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.5626 - val_loss: 0.5850\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5449 - val_loss: 0.5732\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.5287 - val_loss: 0.5618\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.5136 - val_loss: 0.5512\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.5000 - val_loss: 0.5434\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4878 - val_loss: 0.5372\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4764 - val_loss: 0.5283\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.4661 - val_loss: 0.5254\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4572 - val_loss: 0.5191\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.4492 - val_loss: 0.5092\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.4415 - val_loss: 0.5032\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4345 - val_loss: 0.4985\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4281 - val_loss: 0.4945\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.4220 - val_loss: 0.4900\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4170 - val_loss: 0.4859\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.4118 - val_loss: 0.4836\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4075 - val_loss: 0.4813\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4029 - val_loss: 0.4766\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3994 - val_loss: 0.4728\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3955 - val_loss: 0.4698\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3925 - val_loss: 0.4690\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3894 - val_loss: 0.4650\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3868 - val_loss: 0.4634\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3835 - val_loss: 0.4600\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3814 - val_loss: 0.4591\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3794 - val_loss: 0.4556\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3771 - val_loss: 0.4549\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3751 - val_loss: 0.4533\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3735 - val_loss: 0.4513\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3714 - val_loss: 0.4481\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3699 - val_loss: 0.4485\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3679 - val_loss: 0.4507\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3662 - val_loss: 0.4489\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3650 - val_loss: 0.4444\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3635 - val_loss: 0.4443\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3620 - val_loss: 0.4409\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3604 - val_loss: 0.4417\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3592 - val_loss: 0.4414\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3577 - val_loss: 0.4416\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3568 - val_loss: 0.4389\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3556 - val_loss: 0.4376\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3542 - val_loss: 0.4361\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3530 - val_loss: 0.4354\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.3519 - val_loss: 0.4365\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3507 - val_loss: 0.4336\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3500 - val_loss: 0.4331\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3486 - val_loss: 0.4322\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3477 - val_loss: 0.4340\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.3466 - val_loss: 0.4300\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3457 - val_loss: 0.4302\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3446 - val_loss: 0.4289\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3437 - val_loss: 0.4287\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3428 - val_loss: 0.4289\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3421 - val_loss: 0.4288\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3407 - val_loss: 0.4262\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3402 - val_loss: 0.4255\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3394 - val_loss: 0.4242\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3384 - val_loss: 0.4248\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3377 - val_loss: 0.4237\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.3367 - val_loss: 0.4210\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3360 - val_loss: 0.4228\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3349 - val_loss: 0.4221\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3343 - val_loss: 0.4208\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3337 - val_loss: 0.4220\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3326 - val_loss: 0.4208\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3322 - val_loss: 0.4184\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3313 - val_loss: 0.4198\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3307 - val_loss: 0.4186\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3298 - val_loss: 0.4179\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3295 - val_loss: 0.4141\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3285 - val_loss: 0.4165\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3282 - val_loss: 0.4144\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3273 - val_loss: 0.4144\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3265 - val_loss: 0.4137\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3261 - val_loss: 0.4144\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3257 - val_loss: 0.4110\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3247 - val_loss: 0.4150\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3245 - val_loss: 0.4141\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.3239 - val_loss: 0.4113\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3228 - val_loss: 0.4105\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3224 - val_loss: 0.4087\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3222 - val_loss: 0.4131\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3215 - val_loss: 0.4091\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3209 - val_loss: 0.4122\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3205 - val_loss: 0.4089\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.3198 - val_loss: 0.4084\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3194 - val_loss: 0.4084\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.3187 - val_loss: 0.4061\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3184 - val_loss: 0.4095\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3176 - val_loss: 0.4069\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3173 - val_loss: 0.4075\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3165 - val_loss: 0.4068\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3164 - val_loss: 0.4044\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.4042\n",
      "121/121 [==============================] - 0s 521us/step - loss: 0.3363\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 989us/step - loss: 1.8003 - val_loss: 0.6785\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.6278 - val_loss: 0.6150\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.5855 - val_loss: 0.5931\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.5583 - val_loss: 0.5760\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.5347 - val_loss: 0.5603\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 733us/step - loss: 0.5123 - val_loss: 0.5478\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.4924 - val_loss: 0.5342\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.4744 - val_loss: 0.5248\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.4581 - val_loss: 0.5144\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 762us/step - loss: 0.4436 - val_loss: 0.5045\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 736us/step - loss: 0.4312 - val_loss: 0.4952\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 748us/step - loss: 0.4201 - val_loss: 0.4882\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.4113 - val_loss: 0.4810\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.4030 - val_loss: 0.4740\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 736us/step - loss: 0.3962 - val_loss: 0.4720\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3908 - val_loss: 0.4669\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.3855 - val_loss: 0.4635\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3811 - val_loss: 0.4579\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.3771 - val_loss: 0.4576\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.3738 - val_loss: 0.4555\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 743us/step - loss: 0.3714 - val_loss: 0.4507\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 742us/step - loss: 0.3688 - val_loss: 0.4488\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.3664 - val_loss: 0.4461\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.3642 - val_loss: 0.4453\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.3624 - val_loss: 0.4449\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3604 - val_loss: 0.4427\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.3586 - val_loss: 0.4424\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 780us/step - loss: 0.3573 - val_loss: 0.4404\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3554 - val_loss: 0.4411\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.3543 - val_loss: 0.4371\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.3529 - val_loss: 0.4372\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.3519 - val_loss: 0.4363\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3506 - val_loss: 0.4365\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3497 - val_loss: 0.4355\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3481 - val_loss: 0.4334\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3475 - val_loss: 0.4359\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 780us/step - loss: 0.3465 - val_loss: 0.4324\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 764us/step - loss: 0.3454 - val_loss: 0.4348\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.3443 - val_loss: 0.4335\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3438 - val_loss: 0.4343\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.3426 - val_loss: 0.4304\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.3417 - val_loss: 0.4307\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.3411 - val_loss: 0.4323\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3405 - val_loss: 0.4314\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.3396 - val_loss: 0.4285\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.3389 - val_loss: 0.4290\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.3380 - val_loss: 0.4302\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 780us/step - loss: 0.3377 - val_loss: 0.4284\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.3365 - val_loss: 0.4348\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3363 - val_loss: 0.4286\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3355 - val_loss: 0.4314\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.3352 - val_loss: 0.4272\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.3343 - val_loss: 0.4332\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 753us/step - loss: 0.3338 - val_loss: 0.4284\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 748us/step - loss: 0.3329 - val_loss: 0.4260\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 753us/step - loss: 0.3327 - val_loss: 0.4251\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 766us/step - loss: 0.3320 - val_loss: 0.4266\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.3314 - val_loss: 0.4305\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.3308 - val_loss: 0.4268\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.3305 - val_loss: 0.4259\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3301 - val_loss: 0.4251\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3293 - val_loss: 0.4274\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3289 - val_loss: 0.4212\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 742us/step - loss: 0.3282 - val_loss: 0.4221\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3279 - val_loss: 0.4243\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.3278 - val_loss: 0.4233\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3272 - val_loss: 0.4236\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3269 - val_loss: 0.4232\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.3263 - val_loss: 0.4217\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.3259 - val_loss: 0.4279\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.3258 - val_loss: 0.4225\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.3251 - val_loss: 0.4232\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.3244 - val_loss: 0.4223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7f4f7970c940&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7f4f797f0220&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7f4f7970c940&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7f4f797f0220&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7f4f7970c940&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7f4f7970c940&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7f4f7970c940>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7f4f797f0220>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "        callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0013839848917305338, 'n_hidden': 3, 'n_neurons': 21}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 3s 1ms/step - loss: 1.2929 - accuracy: 0.7334 - val_loss: 33.0899 - val_accuracy: 0.0928\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3), \n",
    "              metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.005)\n",
    "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val), callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.001040707"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "expon_lr.rates[np.argmax(expon_lr.losses)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2612 - accuracy: 0.9197 - val_loss: 19.0411 - val_accuracy: 0.9567\n",
      "Epoch 2/1000\n",
      "1719/1719 [==============================] - 2s 955us/step - loss: 0.1123 - accuracy: 0.9655 - val_loss: 16.6154 - val_accuracy: 0.9707\n",
      "Epoch 3/1000\n",
      "1719/1719 [==============================] - 2s 990us/step - loss: 0.0806 - accuracy: 0.9750 - val_loss: 10.2734 - val_accuracy: 0.9816\n",
      "Epoch 4/1000\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.0630 - accuracy: 0.9803 - val_loss: 9.8723 - val_accuracy: 0.9815\n",
      "Epoch 5/1000\n",
      "1719/1719 [==============================] - 2s 968us/step - loss: 0.0551 - accuracy: 0.9828 - val_loss: 8.5534 - val_accuracy: 0.9856\n",
      "Epoch 6/1000\n",
      "1719/1719 [==============================] - 2s 975us/step - loss: 0.0459 - accuracy: 0.9859 - val_loss: 7.8117 - val_accuracy: 0.9866\n",
      "Epoch 7/1000\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 0.0391 - accuracy: 0.9878 - val_loss: 9.7289 - val_accuracy: 0.9837\n",
      "Epoch 8/1000\n",
      "1719/1719 [==============================] - 2s 954us/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 5.9783 - val_accuracy: 0.9887\n",
      "Epoch 9/1000\n",
      "1719/1719 [==============================] - 2s 987us/step - loss: 0.0344 - accuracy: 0.9892 - val_loss: 15.3904 - val_accuracy: 0.9838\n",
      "Epoch 10/1000\n",
      "1719/1719 [==============================] - 2s 973us/step - loss: 0.0314 - accuracy: 0.9906 - val_loss: 11.1844 - val_accuracy: 0.9836\n",
      "Epoch 11/1000\n",
      "1719/1719 [==============================] - 2s 963us/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 6.7815 - val_accuracy: 0.9887\n",
      "Epoch 12/1000\n",
      "1719/1719 [==============================] - 2s 983us/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 12.0727 - val_accuracy: 0.9830\n",
      "Epoch 13/1000\n",
      "1719/1719 [==============================] - 2s 993us/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 5.2905 - val_accuracy: 0.9921\n",
      "Epoch 14/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0264 - accuracy: 0.9926 - val_loss: 10.2944 - val_accuracy: 0.9887\n",
      "Epoch 15/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 9.4789 - val_accuracy: 0.9896\n",
      "Epoch 16/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0320 - accuracy: 0.9922 - val_loss: 9.2083 - val_accuracy: 0.9865\n",
      "Epoch 17/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0264 - accuracy: 0.9924 - val_loss: 5.8631 - val_accuracy: 0.9923\n",
      "Epoch 18/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0276 - accuracy: 0.9922 - val_loss: 7.9569 - val_accuracy: 0.9901\n",
      "Epoch 19/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 4.6309 - val_accuracy: 0.9933\n",
      "Epoch 20/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 7.4198 - val_accuracy: 0.9924\n",
      "Epoch 21/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0205 - accuracy: 0.9947 - val_loss: 5.4762 - val_accuracy: 0.9929\n",
      "Epoch 22/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 6.0336 - val_accuracy: 0.9929\n",
      "Epoch 23/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0193 - accuracy: 0.9949 - val_loss: 9.7771 - val_accuracy: 0.9909\n",
      "Epoch 24/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0173 - accuracy: 0.9952 - val_loss: 12.0501 - val_accuracy: 0.9885\n",
      "Epoch 25/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0205 - accuracy: 0.9950 - val_loss: 10.6035 - val_accuracy: 0.9875\n",
      "Epoch 26/1000\n",
      "1719/1719 [==============================] - 2s 995us/step - loss: 0.0281 - accuracy: 0.9927 - val_loss: 16.7238 - val_accuracy: 0.9855\n",
      "Epoch 27/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 7.8545 - val_accuracy: 0.9925\n",
      "Epoch 28/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0397 - accuracy: 0.9912 - val_loss: 15.7524 - val_accuracy: 0.9848\n",
      "Epoch 29/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 11.4871 - val_accuracy: 0.9884\n",
      "Epoch 30/1000\n",
      "1719/1719 [==============================] - 2s 983us/step - loss: 0.0229 - accuracy: 0.9947 - val_loss: 9.1092 - val_accuracy: 0.9923\n",
      "Epoch 31/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0313 - accuracy: 0.9932 - val_loss: 6.6402 - val_accuracy: 0.9939\n",
      "Epoch 32/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0236 - accuracy: 0.9947 - val_loss: 7.6820 - val_accuracy: 0.9933\n",
      "Epoch 33/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0258 - accuracy: 0.9946 - val_loss: 13.9435 - val_accuracy: 0.9879\n",
      "Epoch 34/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0341 - accuracy: 0.9925 - val_loss: 10.5724 - val_accuracy: 0.9897\n",
      "Epoch 35/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 9.2500 - val_accuracy: 0.9919\n",
      "Epoch 36/1000\n",
      "1719/1719 [==============================] - 2s 994us/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 7.7273 - val_accuracy: 0.9931\n",
      "Epoch 37/1000\n",
      "1719/1719 [==============================] - 2s 956us/step - loss: 0.0273 - accuracy: 0.9940 - val_loss: 9.9725 - val_accuracy: 0.9911\n",
      "Epoch 38/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0148 - accuracy: 0.9965 - val_loss: 7.5246 - val_accuracy: 0.9931\n",
      "Epoch 39/1000\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 9.8928 - val_accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.5), \n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
