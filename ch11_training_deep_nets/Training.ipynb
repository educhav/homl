{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 11:10:09.118908: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-05 11:10:09.146587: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-05 11:10:09.146910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 11:10:09.760158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f5ba99d3fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Use He initialization \n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_uniform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f5ba958f070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use average fan instead of the input fan\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 11:10:10.449368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-05 11:10:10.460426: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Leaky ReLU activation function\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "])\n",
    "\n",
    "# PReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU()\n",
    "])\n",
    "\n",
    "# SELU\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch normalization after activation function\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2551926/3873162892.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  model.layers[1].updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization before activation function\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping (clipvalue or clipnorm)\n",
    "# Clip value will clip if the value is above 1 or below -1\n",
    "# Clip norm will clip if its computed l2 norm is above 1 or below -1\n",
    "# Use clipnorm if you want to preserve the orientation of the gradient vector\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with Keras\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Use cloning if you do not want to effect model_A\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Momentum optimization \n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "# Nesterov Accelerated Gradient\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "# RMSProp\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "# Adam\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, weight_decay=1e-4)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential scheduling\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch/20)\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lr_scheduler \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mLearningRateScheduler(exponential_decay_fn)\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train, [\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], callbacks\u001b[39m=\u001b[39m[lr_scheduler])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scheduling\n",
    "\n",
    "# Reduces learning rate by a factor of 0.5 for every 5 epochs where the val loss does not improve\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(X_train) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      2\u001b[0m learning_rate \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mschedules\u001b[39m.\u001b[39mExponentialDecay(initial_learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, decay_steps\u001b[39m=\u001b[39ms, decay_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "s = 20 * len(X_train) // 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=s, decay_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 and L1 regularization\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l1_l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Avoid redundant code with partial\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", \n",
    "                           kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f5ba81fab60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max-norm regularization\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32,32,3]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "data = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data[0]\n",
    "X_test, y_test = data[1]\n",
    "\n",
    "X_val = X_train[40000:]\n",
    "X_train = X_train[:40000]\n",
    "\n",
    "y_val = y_train[40000:]\n",
    "y_train = y_train[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1250/1250 [==============================] - 8s 4ms/step - loss: 3.7278 - accuracy: 0.2133 - val_loss: 2.0667 - val_accuracy: 0.2423\n",
      "Epoch 2/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.9394 - accuracy: 0.2854 - val_loss: 1.8605 - val_accuracy: 0.3177\n",
      "Epoch 3/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8575 - accuracy: 0.3205 - val_loss: 1.9391 - val_accuracy: 0.2820\n",
      "Epoch 4/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8204 - accuracy: 0.3386 - val_loss: 1.8412 - val_accuracy: 0.3271\n",
      "Epoch 5/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7817 - accuracy: 0.3523 - val_loss: 1.8002 - val_accuracy: 0.3504\n",
      "Epoch 6/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7485 - accuracy: 0.3669 - val_loss: 1.7297 - val_accuracy: 0.3721\n",
      "Epoch 7/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7257 - accuracy: 0.3760 - val_loss: 1.7320 - val_accuracy: 0.3752\n",
      "Epoch 8/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6939 - accuracy: 0.3886 - val_loss: 1.6696 - val_accuracy: 0.4031\n",
      "Epoch 9/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6745 - accuracy: 0.3978 - val_loss: 1.7018 - val_accuracy: 0.3928\n",
      "Epoch 10/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6589 - accuracy: 0.4029 - val_loss: 1.6502 - val_accuracy: 0.4090\n",
      "Epoch 11/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6332 - accuracy: 0.4110 - val_loss: 1.6981 - val_accuracy: 0.3845\n",
      "Epoch 12/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6173 - accuracy: 0.4160 - val_loss: 1.6520 - val_accuracy: 0.4060\n",
      "Epoch 13/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6056 - accuracy: 0.4227 - val_loss: 1.6552 - val_accuracy: 0.4056\n",
      "Epoch 14/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5898 - accuracy: 0.4291 - val_loss: 1.6793 - val_accuracy: 0.4005\n",
      "Epoch 15/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5729 - accuracy: 0.4336 - val_loss: 1.6025 - val_accuracy: 0.4214\n",
      "Epoch 16/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5601 - accuracy: 0.4376 - val_loss: 1.6285 - val_accuracy: 0.4201\n",
      "Epoch 17/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5490 - accuracy: 0.4424 - val_loss: 1.5859 - val_accuracy: 0.4327\n",
      "Epoch 18/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5396 - accuracy: 0.4439 - val_loss: 1.6312 - val_accuracy: 0.4162\n",
      "Epoch 19/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5263 - accuracy: 0.4527 - val_loss: 1.6390 - val_accuracy: 0.4279\n",
      "Epoch 20/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5115 - accuracy: 0.4584 - val_loss: 1.6142 - val_accuracy: 0.4254\n",
      "Epoch 21/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5090 - accuracy: 0.4590 - val_loss: 1.5849 - val_accuracy: 0.4367\n",
      "Epoch 22/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4946 - accuracy: 0.4661 - val_loss: 1.5909 - val_accuracy: 0.4371\n",
      "Epoch 23/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4876 - accuracy: 0.4677 - val_loss: 1.6050 - val_accuracy: 0.4350\n",
      "Epoch 24/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4801 - accuracy: 0.4745 - val_loss: 1.5710 - val_accuracy: 0.4390\n",
      "Epoch 25/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4687 - accuracy: 0.4765 - val_loss: 1.5735 - val_accuracy: 0.4419\n",
      "Epoch 26/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4596 - accuracy: 0.4781 - val_loss: 1.6189 - val_accuracy: 0.4341\n",
      "Epoch 27/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4589 - accuracy: 0.4785 - val_loss: 1.5794 - val_accuracy: 0.4397\n",
      "Epoch 28/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.9853 - accuracy: 0.3088 - val_loss: 1.8312 - val_accuracy: 0.3263\n",
      "Epoch 29/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 2.8172 - accuracy: 0.3328 - val_loss: 1.9812 - val_accuracy: 0.2580\n",
      "Epoch 30/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8857 - accuracy: 0.3012 - val_loss: 1.8545 - val_accuracy: 0.3217\n",
      "Epoch 31/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8147 - accuracy: 0.3314 - val_loss: 1.8089 - val_accuracy: 0.3341\n",
      "Epoch 32/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7738 - accuracy: 0.3468 - val_loss: 1.8103 - val_accuracy: 0.3286\n",
      "Epoch 33/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7479 - accuracy: 0.3580 - val_loss: 1.7769 - val_accuracy: 0.3451\n",
      "Epoch 34/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7248 - accuracy: 0.3716 - val_loss: 1.7937 - val_accuracy: 0.3467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5af674ac20>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(patience=10)\n",
    "exponential_lr = ExponentialLearningRate(factor=1.005)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.0006, beta_1=0.9, beta_2=0.999), \n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"],)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=300, validation_data=(X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(40000, 3072)\n",
      "(10000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, y_train = data[0]\n",
    "X_test, y_test = data[1]\n",
    "X_val = X_train[40000:]\n",
    "X_train = X_train[:40000]\n",
    "y_val = y_train[40000:]\n",
    "y_train = y_train[:40000]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1))\n",
    "X_test = scaler.fit_transform(X_test.reshape(X_test.shape[0],-1))\n",
    "X_val = scaler.fit_transform(X_val.reshape(X_val.shape[0],-1))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "# y_train = y_train[:] / 255.0\n",
    "# y_test = y_test[:] / 255.0\n",
    "# y_val = y_val[:] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12288)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[3072])\n",
    "])\n",
    "\n",
    "for i in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3072)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1250/1250 [==============================] - 8s 4ms/step - loss: 1.9121 - accuracy: 0.3138 - val_loss: 1.8277 - val_accuracy: 0.3478\n",
      "Epoch 2/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7322 - accuracy: 0.3828 - val_loss: 1.7305 - val_accuracy: 0.3837\n",
      "Epoch 3/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.6542 - accuracy: 0.4149 - val_loss: 1.6507 - val_accuracy: 0.4178\n",
      "Epoch 4/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5999 - accuracy: 0.4353 - val_loss: 1.6367 - val_accuracy: 0.4289\n",
      "Epoch 5/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5491 - accuracy: 0.4567 - val_loss: 1.5958 - val_accuracy: 0.4447\n",
      "Epoch 6/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5168 - accuracy: 0.4712 - val_loss: 1.6084 - val_accuracy: 0.4508\n",
      "Epoch 7/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4803 - accuracy: 0.4823 - val_loss: 1.6016 - val_accuracy: 0.4428\n",
      "Epoch 8/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.4455 - accuracy: 0.4922 - val_loss: 1.5616 - val_accuracy: 0.4611\n",
      "Epoch 9/300\n",
      "1250/1250 [==============================] - 4s 4ms/step - loss: 1.4088 - accuracy: 0.5113 - val_loss: 1.5676 - val_accuracy: 0.4575\n",
      "Epoch 10/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3935 - accuracy: 0.5147 - val_loss: 1.5584 - val_accuracy: 0.4423\n",
      "Epoch 11/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3634 - accuracy: 0.5232 - val_loss: 1.5199 - val_accuracy: 0.4608\n",
      "Epoch 12/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3419 - accuracy: 0.5311 - val_loss: 1.5141 - val_accuracy: 0.4778\n",
      "Epoch 13/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.3270 - accuracy: 0.5385 - val_loss: 1.5572 - val_accuracy: 0.4788\n",
      "Epoch 14/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2931 - accuracy: 0.5525 - val_loss: 1.5766 - val_accuracy: 0.4723\n",
      "Epoch 15/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2702 - accuracy: 0.5593 - val_loss: 1.5642 - val_accuracy: 0.4748\n",
      "Epoch 16/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2504 - accuracy: 0.5659 - val_loss: 1.5571 - val_accuracy: 0.4798\n",
      "Epoch 17/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2306 - accuracy: 0.5693 - val_loss: 1.5014 - val_accuracy: 0.4900\n",
      "Epoch 18/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1990 - accuracy: 0.5834 - val_loss: 1.5350 - val_accuracy: 0.4879\n",
      "Epoch 19/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1747 - accuracy: 0.5954 - val_loss: 1.5358 - val_accuracy: 0.4914\n",
      "Epoch 20/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1584 - accuracy: 0.6011 - val_loss: 1.5621 - val_accuracy: 0.4937\n",
      "Epoch 21/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1373 - accuracy: 0.6070 - val_loss: 1.4895 - val_accuracy: 0.4966\n",
      "Epoch 22/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1185 - accuracy: 0.6141 - val_loss: 1.5650 - val_accuracy: 0.4913\n",
      "Epoch 23/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1577 - accuracy: 0.6036 - val_loss: 1.5546 - val_accuracy: 0.4942\n",
      "Epoch 24/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.0794 - accuracy: 0.6284 - val_loss: 1.5482 - val_accuracy: 0.5099\n",
      "Epoch 25/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.0558 - accuracy: 0.6364 - val_loss: 1.6125 - val_accuracy: 0.4863\n",
      "Epoch 26/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.0557 - accuracy: 0.6374 - val_loss: 1.5498 - val_accuracy: 0.4971\n",
      "Epoch 27/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.0298 - accuracy: 0.6481 - val_loss: 1.5590 - val_accuracy: 0.4999\n",
      "Epoch 28/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 402.3138 - accuracy: 0.5469 - val_loss: 1.6116 - val_accuracy: 0.4618\n",
      "Epoch 29/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.2220 - accuracy: 0.5763 - val_loss: 1.5753 - val_accuracy: 0.4750\n",
      "Epoch 30/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1515 - accuracy: 0.5982 - val_loss: 1.6303 - val_accuracy: 0.4780\n",
      "Epoch 31/300\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 1.1131 - accuracy: 0.6117 - val_loss: 1.5573 - val_accuracy: 0.4820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5940159060>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"], \n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=0.001))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=300, validation_data=(X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/activations.py\", line 83, in softmax\n        if x.shape.rank > 1:\n\n    TypeError: Exception encountered when calling layer 'dense_466' (type Dense).\n    \n    '>' not supported between instances of 'NoneType' and 'int'\n    \n    Call arguments received by layer 'dense_466' (type Dense):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      3\u001b[0m np\u001b[39m.\u001b[39margmax(model\u001b[39m.\u001b[39mpredict(X_test), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mpredict(X_test[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      5\u001b[0m \u001b[39m# accuracy_score(y_test, np.argmax(model.predict(X_test)))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileajaxof81.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/eddie/.local/lib/python3.10/site-packages/keras/activations.py\", line 83, in softmax\n        if x.shape.rank > 1:\n\n    TypeError: Exception encountered when calling layer 'dense_466' (type Dense).\n    \n    '>' not supported between instances of 'NoneType' and 'int'\n    \n    Call arguments received by layer 'dense_466' (type Dense):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.argmax(model.predict(X_test), axis=1)\n",
    "model.predict(X_test[0])\n",
    "# accuracy_score(y_test, np.argmax(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/eddie/.local/lib/python3.10/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so: undefined symbol: cudaGetErrorString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlist_physical_devices()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m self_check\n\u001b[1;32m     23\u001b[0m \u001b[39m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m self_check\u001b[39m.\u001b[39;49mpreload_check()\n\u001b[1;32m     28\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m   \u001b[39m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[39m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mCould not find the DLL(s) \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m. TensorFlow requires that these DLLs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mbe installed in a directory that is named in your \u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39mPATH\u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[39m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[39m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[39m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[39m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[39m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mImportError\u001b[0m: /home/eddie/.local/lib/python3.10/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so: undefined symbol: cudaGetErrorString"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cpu_compiler', '/dt9/usr/bin/gcc'),\n",
       "             ('cuda_compute_capabilities',\n",
       "              ['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']),\n",
       "             ('cuda_version', '11.8'),\n",
       "             ('cudnn_version', '8'),\n",
       "             ('is_cuda_build', True),\n",
       "             ('is_rocm_build', False),\n",
       "             ('is_tensorrt_build', True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sysconfig.get_build_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nvidia-cudnn-cu11==8.6.0.163\n",
      "  Downloading nvidia_cudnn_cu11-8.6.0.163-py3-none-manylinux1_x86_64.whl (715.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.7/715.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11\n",
      "  Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cudnn-cu11-8.6.0.163\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-cudnn-cu11==8.6.0.163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
